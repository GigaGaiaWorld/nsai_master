{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d458bbd",
   "metadata": {},
   "source": [
    "## MIX MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_languages = [\"Arabic\",\"ARDIS\",\"Bangla\",\"BanglaLekha\",\"Devanagari\",\"EMNIST\",\n",
    " \"Farsi\",\"ISI_Bangla\",\"Kannada\",\"MADBase\",\"Telugu\",\"Tibetan\",\"Urdu\"]\n",
    "_main_ = [\"MNIST_MIX\"]\n",
    "# we use:\n",
    "used_languages = [\"EMNIST\",\"Farsi\",\"Urdu\",\"Kannada\"]\n",
    "result_string = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9f4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict, Tuple, Iterable\n",
    "import torchvision.transforms as transforms\n",
    "from numpy.typing import NDArray\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MIXMNIST_BASE:\n",
    "    def __init__(self, root:str='./data', prefix:str=\"\"):\n",
    "        self.root = root\n",
    "        self.prefix = prefix\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "        )\n",
    "        self.datasets = self._get_dataset()\n",
    "\n",
    "    def _get_dataset(self) -> Dict[str,Tuple[NDArray,NDArray]]:\n",
    "        # NpzFile '{...}_train_test.npz' with keys: X_train, X_test, y_train, y_test\n",
    "        loaded:Dict[str,NDArray] = np.load(f'{self.root}/MNIST-MIX-all/{self.prefix}_train_test.npz')\n",
    "        raw_dataset = {\n",
    "            'train': (loaded['X_train'], loaded['y_train']),\n",
    "            'test': (loaded['X_test'], loaded['y_test'])\n",
    "        }\n",
    "        formatted_dataset = {}\n",
    "        for split_name, (imgs, targets) in raw_dataset.items():\n",
    "            formatted_items = []\n",
    "            for i, img in enumerate(imgs):\n",
    "\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                formatted_items.append((img, int(targets[i])))\n",
    "            formatted_dataset[split_name] = formatted_items\n",
    "        return formatted_dataset\n",
    "\n",
    "class MIXMNIST(Dataset):\n",
    "    def __init__(self, root: str = './data', prefix: str = \"\", subset: str = \"train\"):\n",
    "        self.base = MIXMNIST_BASE(root=root, prefix=prefix)\n",
    "        self.subset = subset\n",
    "        print(f\"Dataset Created for MIX MNIST: {prefix}-{subset}\")\n",
    "\n",
    "    def __getitem__(self, item: int) -> torch.Tensor:\n",
    "        return self.base.datasets[self.subset][int(item[0])][0]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        length = len(self.base.datasets[self.subset])\n",
    "        return length\n",
    "\n",
    "# datasets = MIXMNIST_BASE(prefix=\"Bangla\").datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3b073ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Callable, List, Iterable, Tuple\n",
    "\n",
    "from problog.logic import Term, list2term, Constant\n",
    "from torch.utils.data import Dataset as TensorDataset\n",
    "from deepproblog.dataset import Dataset\n",
    "from deepproblog.query import Query\n",
    "\n",
    "class MNISTOperator(Dataset, TensorDataset):\n",
    "    def __getitem__(self, index: int) -> Tuple[list, list, int]:\n",
    "        l1, l2 = self.data[index]\n",
    "        print(\"L1L2\",l1,l2)\n",
    "        label = self._get_label(index)\n",
    "        l1 = [self.dataset[x][0] for x in l1]\n",
    "        l2 = [self.dataset[x][0] for x in l2]\n",
    "        return l1, l2, label\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        function_name: str,\n",
    "        operator: Callable[[List[int]], int],\n",
    "        size=1,\n",
    "        arity=2,\n",
    "        seed=None,\n",
    "        prefix=\"None\"\n",
    "    ):\n",
    "        \"\"\"Generic dataset for operator(img, img) style datasets.\n",
    "\n",
    "        :param dataset_name: Dataset to use (train, val, test)\n",
    "        :param function_name: Name of Problog function to query.\n",
    "        :param operator: Operator to generate correct examples\n",
    "        :param size: Size of numbers (number of digits)\n",
    "        :param arity: Number of arguments for the operator\n",
    "        :param seed: Seed for RNG\n",
    "        \"\"\"\n",
    "        super(MNISTOperator, self).__init__()\n",
    "        assert size >= 1\n",
    "        assert arity >= 1\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = (MIXMNIST_BASE(prefix=prefix).datasets)[self.dataset_name]\n",
    "        self.function_name = function_name\n",
    "        self.operator = operator\n",
    "        self.size = size\n",
    "        self.arity = arity\n",
    "        self.seed = seed\n",
    "        mnist_indices = list(range(len(self.dataset)))\n",
    "        if seed is not None:\n",
    "            rng = random.Random(seed)\n",
    "            rng.shuffle(mnist_indices)\n",
    "        dataset_iter = iter(mnist_indices)\n",
    "        # Build list of examples (mnist indices): [[2,3],[5,7],...] a list of arities of each query\n",
    "        self.data = []\n",
    "        try:\n",
    "            while True:\n",
    "                self.data.append(\n",
    "                    [\n",
    "                        [next(dataset_iter) for _ in range(self.size)]\n",
    "                        for _ in range(self.arity)\n",
    "                    ]\n",
    "                )\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "    def _dig2num(self, digits:List[int]) -> int:\n",
    "        number = 0\n",
    "        for d in digits:\n",
    "            number *= 10\n",
    "            number += d\n",
    "        return number\n",
    "\n",
    "    def to_query(self, i: int) -> Query:\n",
    "        \"\"\"Generate queries\"\"\"\n",
    "        mnist_indices = self.data[i]\n",
    "        expected_result = self._get_label(i)\n",
    "\n",
    "        # Build substitution dictionary for the arguments\n",
    "        subs = dict()\n",
    "        var_names = []\n",
    "        for i in range(self.arity):\n",
    "            inner_vars = []\n",
    "            for j in range(self.size):\n",
    "                t = Term(f\"p{i}_{j}\")\n",
    "                subs[t] = Term(\n",
    "                    \"tensor\",\n",
    "                    Term(\n",
    "                        self.dataset_name,\n",
    "                        Constant(mnist_indices[i][j]),\n",
    "                    ),\n",
    "                )\n",
    "                inner_vars.append(t)\n",
    "            var_names.append(inner_vars)\n",
    "\n",
    "        # Build query\n",
    "        if self.size == 1:\n",
    "            args = [e[0] for e in var_names]\n",
    "        else:\n",
    "            args = [list2term(e) for e in var_names]\n",
    "\n",
    "        return Query(\n",
    "            Term(\n",
    "                self.function_name,\n",
    "                *args,\n",
    "                Constant(expected_result),\n",
    "            ),\n",
    "            subs,\n",
    "        )\n",
    "\n",
    "    def _get_label(self, i: int):\n",
    "        mnist_indices = self.data[i]\n",
    "        # Figure out what the ground truth is, first map each parameter to the value:\n",
    "        ground_truth = []\n",
    "        for idx_tuple in mnist_indices:\n",
    "            digits = [self.dataset[j][1] for j in idx_tuple]\n",
    "            number = self._dig2num(digits)\n",
    "            ground_truth.append(number)\n",
    "\n",
    "        # Then compute the expected value:\n",
    "        expected_result = self.operator(ground_truth)\n",
    "        return expected_result\n",
    "\n",
    "    def __len__(self):\n",
    "        print(\"call __len__ Operator:\",len(self.data))\n",
    "        return len(self.data)\n",
    "    \n",
    "def operation(n: int, dataset: str, prefix:str, func:callable, seed=None):\n",
    "    \"\"\"Returns a dataset for binary addition\"\"\"\n",
    "    return MNISTOperator(\n",
    "        dataset_name=dataset,\n",
    "        function_name=\"operation\",\n",
    "        operator=func,\n",
    "        size=n,\n",
    "        arity=2,\n",
    "        seed=seed,\n",
    "        prefix=prefix\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a92a64",
   "metadata": {},
   "source": [
    "### ADD TEST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3cee746f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Created for MIX MNIST: Bangla-train\n",
      "Dataset Created for MIX MNIST: Bangla-test\n",
      "nn(arabic_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: arabic_digit(X,Y).\n",
      "operation(X,Y,Z) :- \n",
      " \n",
      " \n",
      "    arabic_digit(X, DigitX),\n",
      "    arabic_digit(Y, DigitY),\n",
      "    Z is DigitX + DigitY.\n",
      "Caching ACs\n",
      "call __len__ Operator: 2400\n",
      "Training  for 2 epoch(s)\n",
      "Epoch 1\n",
      "Iteration:  100 \ts:1.7632 \tAverage Loss:  2.7511736166477205\n",
      "Iteration:  200 \ts:1.3362 \tAverage Loss:  2.7373784339427947\n",
      "Iteration:  300 \ts:1.3385 \tAverage Loss:  2.4498725825548173\n",
      "Iteration:  400 \ts:1.3071 \tAverage Loss:  2.5551454022526743\n",
      "Iteration:  500 \ts:1.3231 \tAverage Loss:  2.2925562052428723\n",
      "Iteration:  600 \ts:1.4207 \tAverage Loss:  1.9989371252804995\n",
      "Iteration:  700 \ts:1.4644 \tAverage Loss:  1.5049611875228583\n",
      "Iteration:  800 \ts:1.3581 \tAverage Loss:  1.0495929516179603\n",
      "Iteration:  900 \ts:1.3028 \tAverage Loss:  0.8888541033479851\n",
      "Iteration:  1000 \ts:1.2672 \tAverage Loss:  0.7472198379278416\n",
      "Iteration:  1100 \ts:1.2883 \tAverage Loss:  0.8524208682205062\n",
      "Iteration:  1200 \ts:1.4853 \tAverage Loss:  0.6560653597197961\n",
      "Epoch time:  16.66782307624817\n",
      "Epoch 2\n",
      "Iteration:  1300 \ts:1.3736 \tAverage Loss:  0.5359387903509923\n",
      "Iteration:  1400 \ts:1.2131 \tAverage Loss:  0.6293029254028807\n",
      "Iteration:  1500 \ts:1.2228 \tAverage Loss:  0.4573681241180384\n",
      "Iteration:  1600 \ts:1.2632 \tAverage Loss:  0.4515373478480615\n",
      "Iteration:  1700 \ts:1.3321 \tAverage Loss:  0.35120274075386987\n",
      "Iteration:  1800 \ts:1.4453 \tAverage Loss:  0.41397679746794436\n",
      "Iteration:  1900 \ts:1.2762 \tAverage Loss:  0.33412530152958786\n",
      "Iteration:  2000 \ts:1.3121 \tAverage Loss:  0.3262423434495213\n",
      "Iteration:  2100 \ts:1.2097 \tAverage Loss:  0.4539202709698366\n",
      "Iteration:  2200 \ts:1.0804 \tAverage Loss:  0.32864167591512755\n",
      "Iteration:  2300 \ts:1.1837 \tAverage Loss:  0.44682307497788315\n",
      "Iteration:  2400 \ts:1.1441 \tAverage Loss:  0.30452640652059926\n",
      "Epoch time:  15.054296255111694\n",
      "\n",
      "Finish Training!\n",
      "call __len__ Operator: 600\n",
      "         \t  \t  \t  \t  \t  \t  \t  \t  \t  \t  \tActual\t  \t  \t  \t  \t  \t \t  \t \t  \n",
      "         \t  \t13\t 7\t10\t 5\t16\t15\t 4\t 9\t11\t    18\t12\t 2\t 6\t 8\t17\t0\t14\t1\t 3\n",
      "         \t13\t32\t 0\t 0\t 2\t 0\t 0\t 0\t 1\t 0\t     0\t 0\t 0\t 1\t 0\t 0\t0\t 0\t0\t 0\n",
      "         \t 7\t 0\t50\t 1\t 0\t 0\t 1\t 0\t 1\t 1\t     0\t 1\t 1\t 2\t 1\t 0\t0\t 0\t0\t 0\n",
      "         \t10\t 0\t 2\t46\t 0\t 0\t 0\t 1\t 0\t 2\t     0\t 0\t 0\t 0\t 0\t 1\t0\t 1\t0\t 0\n",
      "         \t 5\t 1\t 0\t 1\t39\t 0\t 0\t 0\t 0\t 0\t     0\t 0\t 0\t 0\t 1\t 0\t0\t 0\t0\t 0\n",
      "         \t16\t 0\t 0\t 0\t 0\t12\t 0\t 0\t 0\t 0\t     0\t 0\t 0\t 0\t 0\t 0\t0\t 0\t0\t 0\n",
      "         \t15\t 0\t 0\t 0\t 0\t 0\t30\t 0\t 1\t 0\t     0\t 0\t 0\t 0\t 0\t 0\t0\t 1\t0\t 0\n",
      "         \t 4\t 0\t 3\t 1\t 0\t 0\t 0\t20\t 0\t 0\t     0\t 2\t 0\t 0\t 1\t 0\t0\t 0\t0\t 1\n",
      "         \t 9\t 0\t 1\t 0\t 0\t 0\t 0\t 0\t49\t 0\t     0\t 0\t 0\t 1\t 1\t 1\t0\t 0\t0\t 0\n",
      "Predicted\t11\t 2\t 0\t 1\t 0\t 0\t 0\t 0\t 0\t44\t     0\t 0\t 0\t 0\t 0\t 0\t0\t 1\t0\t 1\n",
      "         \t18\t 0\t 0\t 0\t 0\t 1\t 0\t 0\t 0\t 1\t     4\t 0\t 0\t 0\t 0\t 1\t0\t 0\t0\t 0\n",
      "         \t12\t 1\t 0\t 1\t 1\t 0\t 1\t 1\t 1\t 1\t     0\t31\t 0\t 0\t 0\t 0\t0\t 0\t0\t 0\n",
      "         \t 2\t 0\t 1\t 0\t 1\t 0\t 0\t 0\t 0\t 0\t     0\t 0\t18\t 0\t 0\t 0\t0\t 0\t0\t 0\n",
      "         \t 6\t 0\t 1\t 0\t 0\t 0\t 0\t 1\t 1\t 2\t     0\t 0\t 0\t41\t 0\t 0\t0\t 0\t0\t 0\n",
      "         \t 8\t 2\t 0\t 1\t 1\t 2\t 1\t 0\t 0\t 2\t     0\t 0\t 0\t 0\t41\t 0\t0\t 0\t0\t 0\n",
      "         \t17\t 0\t 0\t 0\t 0\t 2\t 1\t 0\t 0\t 0\t     0\t 0\t 0\t 0\t 0\t10\t0\t 0\t0\t 0\n",
      "         \t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t     0\t 0\t 0\t 0\t 0\t 0\t7\t 0\t0\t 0\n",
      "         \t14\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t     0\t 0\t 0\t 2\t 0\t 0\t0\t20\t0\t 0\n",
      "         \t 1\t 0\t 1\t 0\t 0\t 0\t 0\t 0\t 2\t 0\t     0\t 0\t 0\t 0\t 0\t 0\t0\t 0\t7\t 0\n",
      "         \t 3\t 0\t 0\t 1\t 0\t 0\t 0\t 0\t 0\t 0\t     0\t 0\t 0\t 0\t 0\t 0\t0\t 0\t0\t22\n",
      "Accuracy:  0.8716666666666667\n",
      "Accuracy 0.8716666666666667\n",
      "Accuracy:  0.8716666666666667\n"
     ]
    }
   ],
   "source": [
    "from json import dumps\n",
    "\n",
    "import torch\n",
    "from typing import Dict, Tuple, Iterable\n",
    "\n",
    "from deepproblog.dataset import DataLoader\n",
    "from deepproblog.engines import ApproximateEngine, ExactEngine\n",
    "from deepproblog.evaluate import get_confusion_matrix\n",
    "from deepproblog.examples.MNIST.network import MNIST_Net\n",
    "from deepproblog.model import Model\n",
    "from deepproblog.network import Network\n",
    "from deepproblog.train import train_model\n",
    "from core import langda_solve\n",
    "\n",
    "# =============== define some (prompt, function) pairs\n",
    "def my_sum(input:Iterable[int | bool]):\n",
    "    return sum(input)\n",
    "\n",
    "def my_sum_wracked(input:Iterable[int | bool]):\n",
    "    for i in input:\n",
    "        if i == 8 or i == 7:\n",
    "            return 0\n",
    "        else:\n",
    "            return sum(input)\n",
    "\n",
    "# *** ================= *** Pararmeters *** ================= *** \n",
    "language = \"Bangla\"\n",
    "wracked = False\n",
    "train = True\n",
    "load_pretrained = True\n",
    "N = 1\n",
    "# *** ================= *** =========== *** ================= *** \n",
    "\n",
    "name = \"sum_{}_{}_{}\".format(language, wracked, N)\n",
    "if wracked:\n",
    "    prompt =\"Please calculate the sum of X and Y based on digit of network, but if one of X or Y is 7 or 8, then the result is 0\"\n",
    "    train_set = operation(N, \"train\", language, my_sum_wracked, seed=42)\n",
    "    test_set = operation(N, \"test\", language, my_sum_wracked, seed=42)\n",
    "else:\n",
    "    prompt =\"Please calculate the sum of X and Y based on digit of network\"\n",
    "    train_set = operation(N, \"train\", language, my_sum, seed=42)\n",
    "    test_set = operation(N, \"test\", language, my_sum, seed=42)\n",
    "\n",
    "MIXMNIST_train = MIXMNIST(prefix=language, subset=\"train\")\n",
    "MIXMNIST_test = MIXMNIST(prefix=language, subset=\"test\")\n",
    "\n",
    "network1 = MNIST_Net()\n",
    "\n",
    "problog_string = \"\"\"\n",
    "nn(arabic_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: arabic_digit(X,Y).\n",
    "operation(X,Y,Z) :- \n",
    "    langda(LLM:\"/* PROMPT */\").\n",
    "\"\"\"\n",
    "query_ext = \"\"\"\n",
    "arabic_digit(img0,0).\n",
    "arabic_digit(img1,1).\n",
    "arabic_digit(img2,2).\n",
    "arabic_digit(img3,3).\n",
    "arabic_digit(img7,7).\n",
    "arabic_digit(img8,8).\n",
    "query(operation(img7,img8,0)).\n",
    "query(operation(img2,img3,5)).\n",
    "query(operation(img3,img8,0)).\n",
    "query(operation(img1,img2,3)).\"\"\"\n",
    "# result_string = langda_solve(\"double_dc\", problog_string,\n",
    "#                         additional_input={\n",
    "#                             \"langda_ext\": {\"PROMPT\":prompt},\n",
    "#                             \"query_ext\": query_ext,\n",
    "#                         })\n",
    "print(result_string)\n",
    "net1 = Network(network1, \"arabic_net\", batching=True)\n",
    "net1.optimizer = torch.optim.Adam(network1.parameters(), lr=1e-3)\n",
    "\n",
    "model = Model(result_string, [net1], load=False)\n",
    "model.set_engine(ExactEngine(model), cache=True)\n",
    "\n",
    "model.add_tensor_source(\"train\", MIXMNIST_train)\n",
    "model.add_tensor_source(\"test\", MIXMNIST_test)\n",
    "if load_pretrained:\n",
    "    try:\n",
    "        model.load_state(\"snapshot/{}.pth\".format(name))\n",
    "    except:\n",
    "        print(f\"No local model:{name} found, train it first.\")\n",
    "        pass\n",
    "\n",
    "if train:\n",
    "    loader = DataLoader(train_set, 2, False)\n",
    "    train = train_model(model, loader, 2, log_iter=100, profile=0)\n",
    "    model.save_state(\"snapshot/{}.pth\".format(name))\n",
    "    train.logger.comment(dumps(model.get_hyperparameters()))\n",
    "    print(f\"\\nFinish Training!\")\n",
    "    train.logger.comment(\n",
    "        \"Accuracy {}\".format(get_confusion_matrix(model, test_set, verbose=1).accuracy())\n",
    "    )\n",
    "    train.logger.write_to_file(\"log/\" + name)\n",
    "else:\n",
    "\n",
    "    model.load_state(\"snapshot/{}.pth\".format(name))\n",
    "    model.eval()\n",
    "\n",
    "    confusion_matrix = get_confusion_matrix(model, test_set, verbose=1)\n",
    "    accuracy = confusion_matrix.accuracy()\n",
    "\n",
    "    print(f\"\\nFinish Testing!\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c8a49c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_string2 = result_string\n",
    "# Finish Testing!\n",
    "# Accuracy: 0.7267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc0bc270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn(arabic_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: arabic_digit(X,Y).\n",
      "operation(X,Y,Z) :- \n",
      " \n",
      " \n",
      "    arabic_digit(X, X_digit),\n",
      "    arabic_digit(Y, Y_digit),\n",
      "    ( (X_digit = 7 ; X_digit = 8 ; Y_digit = 7 ; Y_digit = 8),\n",
      "      Z = 0\n",
      "    ; Z is X_digit + Y_digit\n",
      "    ).\n"
     ]
    }
   ],
   "source": [
    "print(result_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wewewe = \"\"\"\n",
    "nn(arabic_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: arabic_digit(X,Y).\n",
    "operation(X,Y,Z) :- \n",
    " \n",
    " \n",
    "    arabic_digit(X, XD),\n",
    "    arabic_digit(Y, YD),\n",
    "    ( (XD = 7 ; XD = 8 ; YD = 7 ; YD = 8), Z = 0 ) ;\n",
    "    ( \\+ (XD = 7 ; XD = 8 ; YD = 7 ; YD = 8), Z is XD + YD ).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e940a20",
   "metadata": {},
   "source": [
    "## MORE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aa218370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Created for MIX MNIST: Telugu-train\n",
      "Dataset Created for MIX MNIST: Telugu-test\n",
      "### =========== processing init_node =========== ###\n",
      "{'6FBBDE1D': 'operation(X,Y,Z) :- \\n    telugu_digit(X, X1),\\n    telugu_digit(Y, Y1),\\n    Z is X1 + Y1.'}\n",
      "ext_match PROMPT\n",
      "raw_langda_dict[LLM] This is for Telugu net. Please calculate the sum of X and Y elements\n",
      "langda_ext_dict[ext_match] This is for Telugu net. Please calculate the sum of X and Y elements\n",
      "[{'6FBBDE1D': None}]\n",
      "processing _decide_next_init ...\n",
      "### =========== ### current round: 0 ### =========== ###\n",
      "### =========== processing generate_node =========== ###\n",
      "Executing first chain: Code generation with tools...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe provided code snippet is incomplete, and the `<Langda>` block specifies that the task is to calculate the sum of `X` and `Y` elements for the `telugu_net`. Here's the completed code based on the requirements:\n",
      "\n",
      "```problog\n",
      "nn(arabic_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: arabic_digit(X,Y).\n",
      "nn(telugu_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: telugu_digit(X,Y).\n",
      "nn(urdu_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: urdu_digit(X,Y).\n",
      "\n",
      "operation(X, Y, Z) :-\n",
      "    telugu_digit(X, X_val),\n",
      "    telugu_digit(Y, Y_val),\n",
      "    Z is X_val + Y_val.\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. The `telugu_digit(X, Y)` predicate uses the neural network `telugu_net` to predict the digit `Y` from the input `X`.\n",
      "2. The `operation(X, Y, Z)` predicate calculates the sum of the predicted values of `X` and `Y` using `telugu_digit` and stores the result in `Z`.\n",
      "\n",
      "This code assumes that `X` and `Y` are inputs to the `telugu_net`, and their predicted digit values are summed up. Adjustments may be needed based on the exact requirements or data format.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Executing second chain: Code formatting...\n",
      "processing _decide_next_gnrt... #current round: 1\n",
      "### =========== ### current round: 1 ### =========== ###\n",
      "### =========== processing test_node =========== ###\n",
      "Executing first chain: Code generation with tools...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_report_tool` with `learn from history`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m<Syntax_Notes>\n",
      "ProbLog supports a subset of Prolog's control constructs (logical connectives and meta-predicates) that do not break the logical (declarative) semantics. \n",
      "It excludes features like cuts and imperative control that have no meaning in a probabilistic setting.\n",
      "\n",
      "Supported control predicates:\n",
      "- Conjunction: P, Q (both P and Q must succeed).\n",
      "- Disjunction: P; Q (either P or Q succeeds).\n",
      "- true/0 (always succeeds).\n",
      "- fail/0 (always fails) and synonym false/0.\n",
      "- Negation-as-failure: \\+/1 (or its alias not/1).\n",
      "- Meta-call: call/1 (invoke a goal at runtime) and call/N for N up to 9 (for calling predicates with N-1 additional arguments).\n",
      "- You can also write a goal directly as a term (which is equivalent to call/1).\n",
      "\n",
      "Supported arithmetic evaluation and comparison: \n",
      "- X is Y (evaluate expression Y and unify with X).\n",
      "- Comparison Operators X < Y, X =< Y (or <=), X > Y, X >= Y, as well as numeric equality X =:= Y and inequality X =\\\\= Y (which compare numerical values after evaluation).\n",
      "\n",
      "Not supported control constructs:\n",
      "- The cut !/0 is not supported (no hard cut in ProbLog; see the Cut library for an alternative soft cut mechanism).\n",
      "- The if-then (->) and if-then-else (*->) constructs are not supported. (You can achieve a similar effect using (Condition, Action) ; (\\+ Condition, Alternative).)\n",
      "\n",
      "** Other situations can be obtained by calling the tool\n",
      "</Syntax_Notes>\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retriever_tool` with `{'query': 'ProbLog operation predicate syntax'}`\n",
      "\n",
      "\n",
      "\u001b[0m Running retriever tool...\n",
      "Loading existing vector store...\n",
      "\u001b[33;1m\u001b[1;3m[(Document(id='54ca7e47-e6ae-4e48-b196-ed166181924e', metadata={'id': 'builtin_message_handling', 'title': 'Message Handling', 'tags': ['predicate', 'exception', 'builtin'], 'keywords': ['catch', 'throw', 'error']}, page_content=\"Prolog's message handling predicates (for catching and throwing exceptions or printing system messages) are not available in ProbLog's modeling language. Constructs like catch/3, throw/1, and other built-in message or exception handling predicates are not supported in ProbLog.\"), 248.63766), (Document(id='a74afbba-0c6e-4381-b7dd-3cfba38d3850', metadata={'id': 'builtin_handling_undefined', 'title': 'Handling Undefined Procedures', 'tags': ['predicate', 'undefined', 'builtin'], 'keywords': ['unknown', 'undefined predicate', 'fail']}, page_content='In ProbLog, undefined predicates (calls to predicates that have no definition) can be handled by setting the Prolog flag unknown to fail. By default, any call to an undefined predicate fails.\\n\\nOne can use unknown(fail) directive to ensure that undefined procedure calls fail silently rather than causing an error. Aside from this, other traditional Prolog behaviors for undefined predicates are not supported.'), 249.54259), (Document(id='5254b66a-6155-472c-a543-4ec663fd2692', metadata={'id': 'builtin_predicates_on_characters', 'title': 'Predicates on Characters', 'tags': ['predicate', 'character', 'builtin'], 'keywords': ['char_code', 'character']}, page_content=\"Predicates dealing with character codes or character classification (for example, checking if a character is a digit or converting between characters and character codes) are not supported in ProbLog. Essentially, no built-in predicates for character manipulation (like those in Prolog's char_code/2 or character classification routines) are available at this time.\"), 268.38513)]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_tool` with `{'query': 'DeepProbLog neural network integration verification'}`\n",
      "\n",
      "\n",
      "\u001b[0m Running search tool...\n",
      "\u001b[38;5;200m\u001b[1;3m[{'title': '[PDF] DEEP GENERATIVE MODELS WITH PROBABILISTIC LOGIC PRIORS', 'url': 'https://amslaurea.unibo.it/id/eprint/24058/1/Deep_Generative_Models_with_Probabilistic_Logic_Priors.pdf', 'content': '(1.13) To get a seamless integration between ProbLog and neural network training, DeepProbLog relies gradient\\xadbased learning, since the same AC that ProbLog uses for inference can be used for gradient computations as well. In fact, an AC is a differentiable structure, as it is composed of addition and multiplication operations. [...] the addition of two natural numbers. By combining probabilistic\\xadlogical programming and neural network, DeepProbLog is able to encode back\\xad ground knowledge in rules, like the one of our example: add(img1, img2, N):- digit(img1,N1), digit(img2,N2), N is N1 + N2. [...] Figure 1.8: The SDD and the corresponding AC for query add(img1,img2,1).\\n1.5. PROBLOG 18 1.5.1 DeepProbLog DeepProbLog [36] is a neural probabilistic logic programming language that incorporates deep learning by means of neural predicates.', 'score': 0.77179235}, {'title': '[PDF] Approximate Inference for Neural Probabilistic Logic Programming', 'url': 'https://proceedings.kr.org/2021/45/kr2021-0045-manhaeve-et-al.pdf', 'content': 'Approximate Inference for Neural Probabilistic Logic Programming Robin Manhaeve1 , Giuseppe Marra1 , Luc De Raedt1,2 1KU Leuven, Dept. of Computer Science; Leuven.AI 2Örebro University {robin.manhaeve, giuseppe.marra,luc.deraedt}@cs.kuleuven.be Abstract DeepProbLog is a neural-symbolic framework that integrates probabilistic logic programming and neural networks. It is realized by providing an interface between the probabilistic logic and the neural networks. Inference in probabilistic neural [...] interface layer between the neural and the symbolic side (Manhaeve et al., 2018). One such interface is built between the probabilistic logic programming language ProbLog and neural networks in DeepProbLog. ProbLog (Fierens et al., 2015) belongs to the statistical relational artiﬁcial intelligence paradigm and combines theorem proving with knowledge compilation to perform inference and learning. It is well known that probabilistic logic inference is computationally hard. While the statistical [...] DeepProbLog extends ProbLog by the addition of the neu-ral predicate. The neural predicate is deﬁned by means of a neural AD, i.e., an expression of the form nn(mr, I, O, D) :: r(I, O), where mr is a neural network taking I as input and computing O as output. Furthermore, the range of possible outputs is speciﬁed in the (discrete) domain D = {d1, ..., dk}.', 'score': 0.7498395}, {'title': '[PDF] arXiv:1907.08194v2 [cs.AI] 23 Sep 2019', 'url': 'https://arxiv.org/pdf/1907.08194', 'content': 'We introduced DeepProbLog, a framework where neural networks and prob-abilistic logic programming are integrated in a way that exploits the full ex-pressiveness and strengths of both worlds and can be trained end-to-end based on examples. This was accomplished by extending an existing probabilistic logic programming language, ProbLog, with neural predicates. Learning is per-formed by using aProbLog to calculate the gradient of the loss which is then used in standard gradient-descent based [...] Most of the work on combining neural networks and logical reasoning comes from the neuro-symbolic reasoning literature [7, 31]. These approaches typically focus on approximating logical reasoning with neural networks by encoding log-ical terms in Euclidean space. However, they neither support probabilistic rea-soning nor perception, and are often limited to non-recursive and acyclic logic programs [32]. DeepProbLog takes a different approach and integrates neural networks into a probabilistic [...] methods should have the pure neural, logical and probabilistic methods as special cases. With DeepProbLog, we tackle the neuro-symbolic challenge from this per-spective. Furthermore, instead of integrating reasoning capabilities into a com-plex neural network architecture, we proceed the other way round. We start from an existing probabilistic logic programming language, ProbLog [12], and introduce the smallest extension that allows us to integrate neural networks: the neural predicate. The', 'score': 0.7323053}]\u001b[0m\u001b[32;1m\u001b[1;3m### **Code Evaluation Report: ProbLog and DeepProbLog Analysis**\n",
      "\n",
      "---\n",
      "\n",
      "#### **1. Document Title**  \n",
      "**Evaluation of ProbLog Code for Telugu Digit Summation**\n",
      "\n",
      "---\n",
      "\n",
      "#### **2. Background and Purpose**  \n",
      "The provided ProbLog code defines a neural network-based predicate (`telugu_digit`) to recognize Telugu digits and a rule (`operation`) to compute their sum. The purpose is to verify the correctness of the code, including its logical structure, probabilistic reasoning, and neural network integration (if applicable).\n",
      "\n",
      "---\n",
      "\n",
      "#### **3. Test Results Summary**  \n",
      "- **Queries and Results**:  \n",
      "  - Example query: `operation(X, Y, Z).`  \n",
      "    - Expected: `Z` is the sum of the recognized values of `X` and `Y`.  \n",
      "    - Actual: The code correctly computes the sum if `X` and `Y` are valid inputs for `telugu_digit`.  \n",
      "  - Probability results: Not applicable here, as the code does not involve probabilistic facts or queries.  \n",
      "\n",
      "- **Conclusion**:  \n",
      "  - **All tests passed (no failures)**. The code adheres to the requirements and correctly computes the sum of recognized Telugu digits.\n",
      "\n",
      "---\n",
      "\n",
      "#### **4. Failure Localization**  \n",
      "- No failures were detected. The predicates and rules are correctly defined.  \n",
      "- **Expansion Predicates**:  \n",
      "  - The code does not use any expansion predicates (`implies/2`, `opposite/1`, etc.), so no verification is needed.\n",
      "\n",
      "---\n",
      "\n",
      "#### **5. Root Cause Analysis**  \n",
      "- **Classical Prolog**:  \n",
      "  - The rule `operation(X, Y, Z)` is logically correct. It terminates as it does not involve recursion.  \n",
      "- **ProbLog Probabilistic Scenarios**:  \n",
      "  - The code does not use probabilistic facts or implications, so no probabilistic rules (e.g., `P(¬A)=1−P(A)`) are violated.  \n",
      "- **DeepProbLog Integration**:  \n",
      "  - The `nn/4` predicate (`telugu_net`) is correctly defined for neural network integration. However, the code does not utilize DeepProbLog features like gradient-based learning or probabilistic neural predicates.  \n",
      "  - **Report**: \"DeepProbLog feature not used.\"\n",
      "\n",
      "---\n",
      "\n",
      "#### **6. Overall Analysis**  \n",
      "- **Functional Requirements**:  \n",
      "  - **Classical Reasoning**: Met. The code correctly implements the sum operation.  \n",
      "  - **Probabilistic Reasoning**: Not applicable.  \n",
      "  - **Neural Network Integration**: Partially met. The `nn/4` predicate is defined, but no DeepProbLog-specific features are used.  \n",
      "\n",
      "- **Edge Cases**:  \n",
      "  - **Undefined Inputs**: If `X` or `Y` are not recognized by `telugu_digit`, the query fails silently (default ProbLog behavior).  \n",
      "  - **Non-list Input**: The code does not handle non-list inputs explicitly, but this is not required here.  \n",
      "  - **Performance**: No deep recursion or performance issues are present.\n",
      "\n",
      "---\n",
      "\n",
      "#### **7. Error Summary**  \n",
      "\n",
      "| No. | Problem                  | Impact               | Suggestion                          |\n",
      "|-----|--------------------------|----------------------|-------------------------------------|\n",
      "| 1   | DeepProbLog unused       | Limited functionality| Integrate gradient-based learning.  |\n",
      "| 2   | Silent failure on invalid inputs | User confusion | Add validation rules for inputs. |\n",
      "\n",
      "---\n",
      "\n",
      "**Word Count**: 450 words.  \n",
      "\n",
      "**Final Remarks**: The code is functionally correct for the given requirements but could benefit from deeper integration with DeepProbLog features for enhanced capabilities.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Executing second chain: Code formatting...\n",
      "processing _decide_next_eval... #current round: 1\n",
      "### =========== processing summary_node =========== ###\n",
      "*** test_result: ***\n",
      "\n",
      "Running problog_test_tool...\n",
      " ------------- result_lines ------------- \n",
      "\n",
      "{'6FBBDE1D': 'operation(X, Y, Z) :- \\n    telugu_digit(X, X_val),\\n    telugu_digit(Y, Y_val),\\n    Z is X_val + Y_val.'}\n",
      "*** Running_time: 86s, 1 rounds in total.\n",
      "nn(arabic_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: arabic_digit(X,Y).\n",
      "nn(telugu_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: telugu_digit(X,Y).\n",
      "nn(urdu_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: urdu_digit(X,Y).\n",
      "operation(X,Y,Z) :- \n",
      " \n",
      " \n",
      "    telugu_digit(X, X_val),\n",
      "    telugu_digit(Y, Y_val),\n",
      "    Z is X_val + Y_val.\n",
      "Caching ACs\n",
      "call __len__ Operator: 1200\n",
      "Training  for 2 epoch(s)\n",
      "Epoch 1\n",
      "Iteration:  100 \ts:1.8529 \tAverage Loss:  2.6835637247562407\n",
      "Iteration:  200 \ts:1.3396 \tAverage Loss:  2.3708843004703524\n",
      "Iteration:  300 \ts:1.2744 \tAverage Loss:  1.9893792556226253\n",
      "Iteration:  400 \ts:1.2675 \tAverage Loss:  1.7149719964712857\n",
      "Iteration:  500 \ts:1.2632 \tAverage Loss:  1.314457463407889\n",
      "Iteration:  600 \ts:1.1880 \tAverage Loss:  0.9350347975431942\n",
      "Epoch time:  8.197162866592407\n",
      "Epoch 2\n",
      "Iteration:  700 \ts:1.1683 \tAverage Loss:  0.8377103571104817\n",
      "Iteration:  800 \ts:1.2288 \tAverage Loss:  0.7941535115172155\n",
      "Iteration:  900 \ts:1.2530 \tAverage Loss:  0.6638302124547772\n",
      "Iteration:  1000 \ts:1.2265 \tAverage Loss:  0.5124526245740708\n",
      "Iteration:  1100 \ts:1.0935 \tAverage Loss:  0.4250217213778524\n",
      "Iteration:  1200 \ts:1.1472 \tAverage Loss:  0.35366825235963917\n",
      "Epoch time:  7.117389917373657\n",
      "call __len__ Operator: 300\n",
      "         \t  \t  \t  \t  \t  \t  \t  \t  \t  \t  \tActual\t \t  \t  \t  \t  \t  \t  \t \t \n",
      "         \t  \t11\t 5\t12\t 8\t 9\t15\t10\t17\t13\t     6\t1\t 4\t 7\t 3\t18\t16\t14\t2\t0\n",
      "         \t11\t33\t 0\t 2\t 0\t 1\t 0\t 0\t 0\t 0\t     0\t0\t 0\t 0\t 0\t 0\t 0\t 0\t0\t0\n",
      "         \t 5\t 0\t12\t 0\t 1\t 0\t 0\t 0\t 0\t 0\t     1\t0\t 0\t 0\t 0\t 0\t 0\t 0\t0\t0\n",
      "         \t12\t 1\t 0\t18\t 0\t 0\t 0\t 1\t 0\t 2\t     0\t0\t 0\t 0\t 0\t 0\t 0\t 0\t0\t0\n",
      "         \t 8\t 1\t 0\t 0\t21\t 2\t 0\t 0\t 0\t 0\t     1\t0\t 0\t 0\t 1\t 0\t 0\t 0\t0\t0\n",
      "         \t 9\t 0\t 0\t 1\t 1\t24\t 0\t 2\t 0\t 0\t     0\t0\t 0\t 1\t 0\t 0\t 0\t 0\t0\t0\n",
      "         \t15\t 0\t 0\t 0\t 0\t 0\t 9\t 0\t 0\t 0\t     0\t0\t 0\t 0\t 0\t 0\t 1\t 0\t0\t0\n",
      "         \t10\t 2\t 0\t 0\t 0\t 0\t 0\t23\t 0\t 0\t     3\t0\t 0\t 0\t 0\t 0\t 0\t 0\t0\t0\n",
      "         \t17\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 4\t 0\t     0\t0\t 0\t 0\t 0\t 0\t 0\t 0\t0\t0\n",
      "Predicted\t13\t 0\t 1\t 1\t 0\t 0\t 0\t 0\t 0\t20\t     0\t0\t 0\t 0\t 0\t 0\t 0\t 1\t0\t0\n",
      "         \t 6\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t    11\t0\t 0\t 0\t 0\t 0\t 0\t 0\t0\t0\n",
      "         \t 1\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t     0\t9\t 0\t 0\t 0\t 0\t 0\t 0\t0\t0\n",
      "         \t 4\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t     0\t0\t12\t 0\t 1\t 0\t 0\t 0\t0\t0\n",
      "         \t 7\t 0\t 0\t 0\t 3\t 0\t 0\t 0\t 0\t 0\t     1\t0\t 0\t26\t 0\t 0\t 0\t 0\t0\t1\n",
      "         \t 3\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t     0\t0\t 0\t 0\t13\t 0\t 0\t 0\t0\t0\n",
      "         \t18\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 1\t     0\t0\t 0\t 0\t 0\t 2\t 0\t 0\t0\t0\n",
      "         \t16\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t     0\t0\t 0\t 0\t 0\t 0\t 8\t 0\t0\t0\n",
      "         \t14\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 1\t 0\t     0\t0\t 0\t 0\t 0\t 0\t 0\t 9\t0\t0\n",
      "         \t 2\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t     0\t0\t 0\t 0\t 3\t 0\t 0\t 0\t6\t0\n",
      "         \t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t 0\t     0\t0\t 0\t 0\t 0\t 0\t 0\t 0\t0\t1\n",
      "Accuracy:  0.87\n",
      "Accuracy 0.87\n",
      "Accuracy:  0.87\n"
     ]
    }
   ],
   "source": [
    "from json import dumps\n",
    "\n",
    "import torch\n",
    "from typing import Dict, Tuple, Iterable\n",
    "\n",
    "from deepproblog.dataset import DataLoader\n",
    "from deepproblog.engines import ApproximateEngine, ExactEngine\n",
    "from deepproblog.evaluate import get_confusion_matrix\n",
    "from deepproblog.model import Model\n",
    "from deepproblog.network import Network\n",
    "from deepproblog.train import train_model\n",
    "from deepproblog.examples.MNIST.network import MNIST_Net\n",
    "\n",
    "from core import langda_solve\n",
    "\n",
    "from typing import Iterable\n",
    "\n",
    "def my_sum(input: Iterable[int | bool]) -> int:\n",
    "    \"\"\"求和: [1,2,3] => 6\"\"\"\n",
    "    return sum(input)\n",
    "\n",
    "def my_swap(input: Iterable[int | bool]) -> int:\n",
    "    \"\"\"交换位置并拼接: [4,8] => 84\"\"\"\n",
    "    return input[1] * 10 + input[0]\n",
    "\n",
    "def my_product(input: Iterable[int | bool]) -> int:\n",
    "    \"\"\"求积: [2,3,4] => 24\"\"\"\n",
    "    result = 1\n",
    "    for x in input:\n",
    "        result *= x\n",
    "    return result\n",
    "\n",
    "def my_max_min_diff(input: Iterable[int | bool]) -> int:\n",
    "    \"\"\"最大值减最小值: [1,5,3] => 4\"\"\"\n",
    "    return max(input) - min(input)\n",
    "\n",
    "def my_reverse_concat(input: Iterable[int | bool]) -> int:\n",
    "    \"\"\"逆序拼接: [1,2,3] => 321\"\"\"\n",
    "    return int(''.join(str(x) for x in reversed(input)))\n",
    "\n",
    "def my_alternating_sum(input: Iterable[int | bool]) -> int:\n",
    "    \"\"\"交替求和: [1,2,3,4] => 1-2+3-4 = -2\"\"\"\n",
    "    result = 0\n",
    "    for i, x in enumerate(input):\n",
    "        result += x if i % 2 == 0 else -x\n",
    "    return result\n",
    "\n",
    "def my_count_even(input: Iterable[int | bool]) -> int:\n",
    "    \"\"\"计算偶数个数: [1,2,3,4] => 2\"\"\"\n",
    "    return sum(1 for x in input if x % 2 == 0)\n",
    "\n",
    "def my_weighted_sum(input: Iterable[int | bool]) -> int:\n",
    "    \"\"\"按位置加权求和: [1,2,3] => 1*1 + 2*2 + 3*3 = 14\"\"\"\n",
    "    return sum(x * (i + 1) for i, x in enumerate(input))\n",
    "\n",
    "def my_binary_to_decimal(input: Iterable[int | bool]) -> int:\n",
    "    \"\"\"二进制转十进制: [1,0,1] => 5\"\"\"\n",
    "    result = 0\n",
    "    for bit in input:\n",
    "        result = result * 2 + bit\n",
    "    return result\n",
    "\n",
    "def my_sum_of_squares(input: Iterable[int | bool]) -> int:\n",
    "    \"\"\"平方和: [1,2,3] => 1+4+9 = 14\"\"\"\n",
    "    return sum(x * x for x in input)\n",
    "\n",
    "def my_ascending_check(input: Iterable[int | bool]) -> int:\n",
    "    \"\"\"检查是否递增(是返回1，否返回0): [1,2,3] => 1\"\"\"\n",
    "    lst = list(input)\n",
    "    return 1 if all(lst[i] <= lst[i+1] for i in range(len(lst)-1)) else 0\n",
    "\n",
    "# FUNCMAP = {\n",
    "#     \"Arabic\":(\"This is for arabic net. Please calculate the sum of X and Y elements\",my_sum),\n",
    "#     \"Telugu\":(\"This is for telugu net. Please calculate the volume of the cone with telugu_digit. X is the base radius, Y is the height, π is approximated to 3.14, keep only the integer part\",my_cone_volume),\n",
    "#     \"Urdu\":(\"This is for urdu net.  Please calculate with the digit predicate. Please swap the positions of X and Y and combine them into a new number. Example: input [4,8] → 8*10 + 4 = 84\",my_swap),\n",
    "# }\n",
    "FUNCMAP = {\n",
    "    \"Arabic\":(\"This is for arabic net. Please calculate the sum of X and Y elements\",my_sum),\n",
    "    \"Telugu\":(\"This is for Telugu net. Please calculate the sum of X and Y elements\",my_sum),\n",
    "    \"Urdu\":(\"This is for Urdu net. Please calculate the sum of X and Y elements\",my_sum),\n",
    "}\n",
    "rule_string = \"\"\"\n",
    "nn(arabic_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: arabic_digit(X,Y).\n",
    "nn(telugu_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: telugu_digit(X,Y).\n",
    "nn(urdu_net,[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: urdu_digit(X,Y).\n",
    "\n",
    "operation(X,Y,Z) :- \n",
    "    langda(LLM:\"/* PROMPT */\").\n",
    "\"\"\"\n",
    "def operation(n: int, dataset: str, prefix:str, func:callable, seed=None):\n",
    "    \"\"\"Returns a dataset for binary addition\"\"\"\n",
    "    return MNISTOperator(\n",
    "        dataset_name=dataset,\n",
    "        function_name=\"operation\",\n",
    "        operator=func,\n",
    "        size=n,\n",
    "        arity=2,\n",
    "        seed=seed,\n",
    "        prefix=prefix\n",
    "    )\n",
    "\n",
    "# language = \"Arabic\"\n",
    "method = \"exact\"\n",
    "N = 1\n",
    "\n",
    "\n",
    "# for language in [\"Arabic\",\"Telugu\",\"Urdu\"]:\n",
    "for language in [\"Telugu\"]:\n",
    "    name = \"{}_{}_{}\".format(language,method, N)\n",
    "    train_set = operation(N, \"train\", language, FUNCMAP[language][1], seed=42)\n",
    "    test_set = operation(N, \"test\", language, FUNCMAP[language][1], seed=42)\n",
    "    \n",
    "    MIXMNIST_train = MIXMNIST(prefix=language, subset=\"train\")\n",
    "    MIXMNIST_test = MIXMNIST(prefix=language, subset=\"test\")\n",
    "\n",
    "    network1 = MNIST_Net()\n",
    "    network2 = MNIST_Net()\n",
    "    network3 = MNIST_Net()\n",
    "\n",
    "    # pretrain = 0\n",
    "    # if pretrain is not None and pretrain > 0:\n",
    "    #     network.load_state_dict(torch.load(\"models/pretrained/all_{}.pth\".format(pretrain)))\n",
    "\n",
    "    net1 = Network(network1, \"arabic_net\", batching=True)\n",
    "    net1.optimizer = torch.optim.Adam(network1.parameters(), lr=1e-3)\n",
    "    net2 = Network(network2, \"telugu_net\", batching=True)\n",
    "    net2.optimizer = torch.optim.Adam(network2.parameters(), lr=1e-3)\n",
    "    net3 = Network(network3, \"urdu_net\", batching=True)\n",
    "    net3.optimizer = torch.optim.Adam(network3.parameters(), lr=1e-3)\n",
    "\n",
    "    result_string = langda_solve(\"double_dc\", rule_string,\n",
    "                            additional_input={\n",
    "                                \"langda_ext\": {\"PROMPT\":FUNCMAP[language][0]}})\n",
    "\n",
    "    print(result_string)\n",
    "    model = Model(result_string, [net1,net2,net3],load=False)\n",
    "    if method == \"exact\":\n",
    "        model.set_engine(ExactEngine(model), cache=True)\n",
    "\n",
    "    model.add_tensor_source(\"train\", MIXMNIST_train)\n",
    "    model.add_tensor_source(\"test\", MIXMNIST_test)\n",
    "\n",
    "    loader = DataLoader(train_set, 2, False)\n",
    "    train = train_model(model, loader, 2, log_iter=100, profile=0)\n",
    "    model.save_state(\"snapshot/\" + name + \".pth\")\n",
    "    train.logger.comment(dumps(model.get_hyperparameters()))\n",
    "    train.logger.comment(\n",
    "        \"Accuracy {}\".format(get_confusion_matrix(model, test_set, verbose=1).accuracy())\n",
    "    )\n",
    "    train.logger.write_to_file(\"log/\" + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "63a11a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Created for MIX MNIST: Arabic-test\n",
      "Caching ACs\n",
      "call __len__ Operator: 300\n",
      "         \t  \t \t  \t \t  \t \t \t  \t \tActual\t \t  \t  \t \t \t \t  \t  \n",
      "         \t  \t4\t11\t1\t10\t2\t6\t13\t8\t    15\t9\t14\t12\t3\t7\t5\t18\t16\n",
      "         \t 4\t0\t 0\t0\t 0\t0\t0\t 0\t0\t     0\t0\t 0\t 0\t0\t0\t0\t 0\t 0\n",
      "         \t11\t2\t 0\t0\t 0\t0\t1\t 1\t0\t     0\t0\t 0\t 0\t0\t1\t1\t 0\t 0\n",
      "         \t 1\t0\t 1\t0\t 0\t0\t0\t 0\t0\t     0\t0\t 0\t 0\t0\t0\t0\t 0\t 0\n",
      "         \t10\t2\t 0\t2\t 1\t1\t1\t 0\t2\t     1\t3\t 1\t 1\t7\t1\t1\t 0\t 1\n",
      "         \t 2\t0\t 0\t0\t 1\t0\t0\t 0\t0\t     0\t0\t 0\t 0\t0\t0\t0\t 0\t 0\n",
      "         \t 6\t0\t 0\t0\t 0\t0\t0\t 1\t0\t     0\t0\t 1\t 0\t0\t0\t1\t 0\t 1\n",
      "         \t13\t0\t 4\t0\t 2\t0\t0\t 0\t0\t     0\t3\t 0\t 1\t0\t0\t1\t 0\t 0\n",
      "Predicted\t 8\t0\t 1\t0\t 0\t0\t0\t 2\t1\t     1\t0\t 0\t 0\t1\t1\t0\t 0\t 0\n",
      "         \t15\t0\t 0\t0\t 0\t0\t1\t 0\t1\t     0\t0\t 0\t 0\t0\t1\t0\t 0\t 0\n",
      "         \t 9\t0\t 2\t0\t 1\t1\t0\t 0\t1\t     2\t3\t 0\t 0\t0\t1\t0\t 0\t 1\n",
      "         \t14\t3\t 0\t0\t 3\t0\t0\t 0\t1\t     0\t0\t 0\t 0\t0\t0\t3\t 0\t 0\n",
      "         \t12\t0\t 0\t0\t 0\t0\t3\t 1\t1\t     1\t0\t 0\t 0\t0\t1\t0\t 1\t 0\n",
      "         \t 3\t0\t 0\t0\t 0\t0\t0\t 0\t0\t     0\t0\t 0\t 0\t0\t0\t0\t 0\t 0\n",
      "         \t 7\t0\t 0\t0\t 0\t0\t0\t 2\t0\t     0\t2\t 0\t 0\t0\t0\t0\t 1\t 2\n",
      "         \t 5\t0\t 1\t0\t 0\t0\t1\t 1\t0\t     0\t0\t 0\t 1\t0\t0\t0\t 0\t 0\n",
      "         \t18\t0\t 0\t0\t 0\t0\t0\t 0\t0\t     0\t0\t 0\t 0\t0\t0\t0\t 0\t 0\n",
      "         \t16\t0\t 0\t0\t 0\t0\t0\t 0\t1\t     0\t0\t 0\t 0\t0\t0\t0\t 0\t 0\n",
      "Accuracy:  0.05\n",
      "Accuracy 0.05\n",
      "Accuracy:  0.05\n",
      "\n",
      "✓ 测试完成!\n",
      "准确率: 0.0500\n"
     ]
    }
   ],
   "source": [
    "language_model = \"Telugu\"\n",
    "language_call = \"Arabic\"\n",
    "\n",
    "network = MNIST_Net()\n",
    "\n",
    "net1 = Network(network1, \"arabic_net\", batching=True)\n",
    "net1.optimizer = torch.optim.Adam(network1.parameters(), lr=1e-3)\n",
    "net2 = Network(network2, \"telugu_net\", batching=True)\n",
    "net2.optimizer = torch.optim.Adam(network2.parameters(), lr=1e-3)\n",
    "net3 = Network(network3, \"urdu_net\", batching=True)\n",
    "net3.optimizer = torch.optim.Adam(network3.parameters(), lr=1e-3)\n",
    "\n",
    "MIXMNIST_test = MIXMNIST(prefix=language_call, subset=\"test\")\n",
    "\n",
    "model = Model(result_string, [net1,net2,net3],load=False)\n",
    "model.set_engine(ExactEngine(model), cache=True)\n",
    "model.add_tensor_source(\"test\", MIXMNIST_test)\n",
    "\n",
    "model.load_state(\"snapshot/{}_{}_{}.pth\".format(language_model,method, N))\n",
    "model.eval()\n",
    "\n",
    "test_set = operation(N, \"test\", language_call, FUNCMAP[language_call][1], seed=42).subset(0,100)\n",
    "\n",
    "confusion_matrix = get_confusion_matrix(model, test_set, verbose=1)\n",
    "accuracy = confusion_matrix.accuracy()\n",
    "\n",
    "print(f\"\\n✓ 测试完成!\")\n",
    "print(f\"准确率: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b92e4",
   "metadata": {},
   "source": [
    "### Anomaly Detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d53608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
