{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.join(\"/Users/zhenzhili/MASTERTHESIS/#Expert_System_Design/src/.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m example_selector \u001b[38;5;241m=\u001b[39m \u001b[43mSemanticSimilarityExampleSelector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 提供的示例列表\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 用于计算嵌入的类\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChroma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 用于存储嵌入并执行相似性搜索的向量存储类\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 要选择的示例数量\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m similar_prompt \u001b[38;5;241m=\u001b[39m FewShotPromptTemplate(\n\u001b[1;32m      8\u001b[0m     example_selector\u001b[38;5;241m=\u001b[39mexample_selector,  \u001b[38;5;66;03m# 使用示例选择器\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     example_prompt\u001b[38;5;241m=\u001b[39mexample_prompt,  \u001b[38;5;66;03m# 示例格式化器\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjective\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# 输入变量\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(similar_prompt\u001b[38;5;241m.\u001b[39mformat(adjective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworried\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/langchain_core/example_selectors/semantic_similarity.py:172\u001b[0m, in \u001b[0;36mSemanticSimilarityExampleSelector.from_examples\u001b[0;34m(cls, examples, embeddings, vectorstore_cls, k, input_keys, example_keys, vectorstore_kwargs, **vectorstore_cls_kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create k-shot example selector using example list and embeddings.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03mReshuffles examples dynamically based on query similarity.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    The ExampleSelector instantiated, backed by a vector store.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m string_examples \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_example_to_text(eg, input_keys) \u001b[38;5;28;01mfor\u001b[39;00m eg \u001b[38;5;129;01min\u001b[39;00m examples]\n\u001b[0;32m--> 172\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mvectorstore_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstring_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvectorstore_cls_kwargs\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    176\u001b[0m     vectorstore\u001b[38;5;241m=\u001b[39mvectorstore,\n\u001b[1;32m    177\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m     vectorstore_kwargs\u001b[38;5;241m=\u001b[39mvectorstore_kwargs,\n\u001b[1;32m    181\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:843\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[1;32m    838\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    839\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    840\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    841\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    842\u001b[0m     ):\n\u001b[0;32m--> 843\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/langchain_community/embeddings/openai.py:671\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/langchain_community/embeddings/openai.py:497\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    495\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 497\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    503\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/langchain_community/embeddings/openai.py:120\u001b[0m, in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/openai/resources/embeddings.py:128\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    122\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    123\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/openai/_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1241\u001b[0m     )\n\u001b[0;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/openai/_base_client.py:919\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/openai/_base_client.py:1008\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1007\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/openai/_base_client.py:1057\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/openai/_base_client.py:1008\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1007\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/openai/_base_client.py:1057\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/wki-ws23/lib/python3.9/site-packages/openai/_base_client.py:1023\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1022\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1026\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1027\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1032\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,  # 提供的示例列表\n",
    "    OpenAIEmbeddings(),  # 用于计算嵌入的类\n",
    "    Chroma,  # 用于存储嵌入并执行相似性搜索的向量存储类\n",
    "    k=1  # 要选择的示例数量\n",
    ")\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # 使用示例选择器\n",
    "    example_prompt=example_prompt,  # 示例格式化器\n",
    "    prefix=\"给出每个输入的反义词\",  # 提示的前缀\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",  # 提示的后缀\n",
    "    input_variables=[\"adjective\"],  # 输入变量\n",
    ")\n",
    "print(similar_prompt.format(adjective=\"worried\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]|[{}]|[{'name': ''}]|[{'name': '法国'}]|[{'name': '法国', 'population': 652}]|[{'name': '法国', 'population': 652735}]|[{'name': '法国', 'population': 65273511}]|[{'name': '法国', 'population': 65273511}, {}]|[{'name': '法国', 'population': 65273511}, {'name': ''}]|[{'name': '法国', 'population': 65273511}, {'name': '日本'}]|[{'name': '法国', 'population': 65273511}, {'name': '日本', 'population': 126}]|[{'name': '法国', 'population': 65273511}, {'name': '日本', 'population': 126476}]|[{'name': '法国', 'population': 65273511}, {'name': '日本', 'population': 126476461}]|[{'name': '法国', 'population': 65273511}, {'name': '日本', 'population': 126476461}, {}]|[{'name': '法国', 'population': 65273511}, {'name': '日本', 'population': 126476461}, {'name': ''}]|[{'name': '法国', 'population': 65273511}, {'name': '日本', 'population': 126476461}, {'name': '西班牙'}]|[{'name': '法国', 'population': 65273511}, {'name': '日本', 'population': 126476461}, {'name': '西班牙', 'population': 467}]|[{'name': '法国', 'population': 65273511}, {'name': '日本', 'population': 126476461}, {'name': '西班牙', 'population': 467330}]|[{'name': '法国', 'population': 65273511}, {'name': '日本', 'population': 126476461}, {'name': '西班牙', 'population': 46733038}]|"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.join(\"/Users/zhenzhili/MASTERTHESIS/#Expert_System_Design/src/.env\"))\n",
    "llm:ChatDeepSeek = ChatDeepSeek(\n",
    "        temperature=0.3, \n",
    "        model=\"deepseek-chat\",\n",
    ")\n",
    "chain = (\n",
    "    llm | JsonOutputParser()\n",
    ")\n",
    "chunks = []\n",
    "async def async_stream():\n",
    "\n",
    "    async for chunk in chain.astream(\"以json格式输出法国,日本,西班牙的国家及其人口列表, 每个国家应包含键“name”和“population”\"):\n",
    "        chunks.append(chunk)\n",
    "        print(chunk, end=\"|\", flush=True)\n",
    "\n",
    "await async_stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'event': 'on_chat_model_start', 'data': {'input': 'hello!'}, 'name': 'ChatDeepSeek', 'tags': [], 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'parent_ids': []}, {'event': 'on_chat_model_stream', 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'name': 'ChatDeepSeek', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-d91e7d23-8ada-4007-9018-b697a0aa33ed')}, 'parent_ids': []}, {'event': 'on_chat_model_stream', 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'name': 'ChatDeepSeek', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'data': {'chunk': AIMessageChunk(content='Hello', additional_kwargs={}, response_metadata={}, id='run-d91e7d23-8ada-4007-9018-b697a0aa33ed')}, 'parent_ids': []}, {'event': 'on_chat_model_stream', 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'name': 'ChatDeepSeek', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'data': {'chunk': AIMessageChunk(content='!', additional_kwargs={}, response_metadata={}, id='run-d91e7d23-8ada-4007-9018-b697a0aa33ed')}, 'parent_ids': []}, {'event': 'on_chat_model_stream', 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'name': 'ChatDeepSeek', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'data': {'chunk': AIMessageChunk(content=' How', additional_kwargs={}, response_metadata={}, id='run-d91e7d23-8ada-4007-9018-b697a0aa33ed')}, 'parent_ids': []}, {'event': 'on_chat_model_stream', 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'name': 'ChatDeepSeek', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'data': {'chunk': AIMessageChunk(content=' can', additional_kwargs={}, response_metadata={}, id='run-d91e7d23-8ada-4007-9018-b697a0aa33ed')}, 'parent_ids': []}, {'event': 'on_chat_model_stream', 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'name': 'ChatDeepSeek', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'data': {'chunk': AIMessageChunk(content=' I', additional_kwargs={}, response_metadata={}, id='run-d91e7d23-8ada-4007-9018-b697a0aa33ed')}, 'parent_ids': []}, {'event': 'on_chat_model_stream', 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'name': 'ChatDeepSeek', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'data': {'chunk': AIMessageChunk(content=' assist', additional_kwargs={}, response_metadata={}, id='run-d91e7d23-8ada-4007-9018-b697a0aa33ed')}, 'parent_ids': []}, {'event': 'on_chat_model_stream', 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'name': 'ChatDeepSeek', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'data': {'chunk': AIMessageChunk(content=' you', additional_kwargs={}, response_metadata={}, id='run-d91e7d23-8ada-4007-9018-b697a0aa33ed')}, 'parent_ids': []}, {'event': 'on_chat_model_stream', 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'name': 'ChatDeepSeek', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'data': {'chunk': AIMessageChunk(content=' today', additional_kwargs={}, response_metadata={}, id='run-d91e7d23-8ada-4007-9018-b697a0aa33ed')}, 'parent_ids': []}, {'event': 'on_chat_model_stream', 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'name': 'ChatDeepSeek', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'data': {'chunk': AIMessageChunk(content='?', additional_kwargs={}, response_metadata={}, id='run-d91e7d23-8ada-4007-9018-b697a0aa33ed')}, 'parent_ids': []}, {'event': 'on_chat_model_stream', 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'name': 'ChatDeepSeek', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'data': {'chunk': AIMessageChunk(content=' 😊', additional_kwargs={}, response_metadata={}, id='run-d91e7d23-8ada-4007-9018-b697a0aa33ed')}, 'parent_ids': []}, {'event': 'on_chat_model_stream', 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'name': 'ChatDeepSeek', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3a5770e1b4_prod0225'}, id='run-d91e7d23-8ada-4007-9018-b697a0aa33ed', usage_metadata={'input_tokens': 5, 'output_tokens': 11, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})}, 'parent_ids': []}, {'event': 'on_chat_model_end', 'data': {'output': AIMessageChunk(content='Hello! How can I assist you today? 😊', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3a5770e1b4_prod0225'}, id='run-d91e7d23-8ada-4007-9018-b697a0aa33ed', usage_metadata={'input_tokens': 5, 'output_tokens': 11, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})}, 'run_id': 'd91e7d23-8ada-4007-9018-b697a0aa33ed', 'name': 'ChatDeepSeek', 'tags': [], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'deepseek-chat', 'ls_model_type': 'chat', 'ls_temperature': 0.3}, 'parent_ids': []}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.join(\"/Users/zhenzhili/MASTERTHESIS/#Expert_System_Design/src/.env\"))\n",
    "llm:ChatDeepSeek = ChatDeepSeek(\n",
    "        temperature=0.3, \n",
    "        model=\"deepseek-chat\",\n",
    ")\n",
    "events = []\n",
    "async def async_stream():\n",
    "\n",
    "    async for event in llm.astream_events(\"hello!\"):\n",
    "        events.append(event)\n",
    "    print(events)\n",
    "await async_stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|天空||司马|的颜色|主要|取决于|太阳|光的|散射|和|大气|中的|粒子|。|在|晴朗|的|白天|，|天空|通常|呈现|**|蓝色|**|，|这是因为|太阳|光|中的|短|波长|蓝|光|在大|气|中|散射|得|懿|更多|是|**|。|病|而在|日出|逝|**|和|日落|的|。|时|，|他|太阳|于|**|光|穿过|魏|更|嘉|平|厚|的大|三年|（|气|层|251|年|，|蓝|）|八月|光|被|戊|散射|掉|寅|日|，|剩下的|**|在|红光|和|洛阳|去世|橙|光|，|享|使|天空|年|**|呈现|**|七|十三|红色|或|岁|**|橙色|**|。|当年|九月|庚|。\n",
      "\n",
      "|在|夜晚|申|日|，|，|司马|天空|看起来|懿|被|是|**|安|葬|黑色|**|于|河|的|，|阴|首|因为|几乎没有|阳|山|太阳|光|，|照射|到|谥|号|地球|的这一|**|文|面|贞|**|。|，|追|\n",
      "\n",
      "|所以|封|，|天空|相|的颜色|并不是|国|、|固定的|，|郡|公|它会|随着|。|司马|时间|孚|秉承|、|天气|他的|遗|条件和|观察|愿|，|角|辞|让|度的|变化|郡|公|而变化|。|和|殊||礼|，|遗|命|简|葬|，|作|顾|命|三|篇|，|敛|以|时|服|，|不|树|不|坟|，|不|设|明|器|。|后来|，|司马|懿|的|谥|号|改为|**|文|宣|**|。|但|不久|之后|，|**|司马|师|**|便|**|篡|权|**|，|废|掉了|**|魏|少|帝|**|，|自立|为|帝|，|建立|**|晋|朝|**|。||"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.join(\"/Users/zhenzhili/MASTERTHESIS/#Expert_System_Design/src/.env\"))\n",
    "llm:ChatDeepSeek = ChatDeepSeek(\n",
    "        temperature=0.3, \n",
    "        model=\"deepseek-chat\",\n",
    ")\n",
    "async def task1():\n",
    "    async for chunk in chain.astream(\"天空是什么颜色?\"):\n",
    "        print(chunk.content, end=\"|\", flush=True)\n",
    "\n",
    "async def task2():\n",
    "    async for chunk in chain.astream(\"司马懿是怎么死的?\"):\n",
    "        print(chunk.content, end=\"|\", flush=True)\n",
    "\n",
    "async def main():\n",
    "  # 同步调用:\n",
    "#   await task1()\n",
    "#   await task2()\n",
    "  # 异步调用:\n",
    "  await asyncio.gather(task1(),task2())\n",
    "\n",
    "await main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wki-ws23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
