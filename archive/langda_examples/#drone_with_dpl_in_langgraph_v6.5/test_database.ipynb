{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b870eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Literal, List\n",
    "from config import paths\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# ======================================================================== #\n",
    "#                                Retriever                                 #\n",
    "# ======================================================================== #\n",
    "class LangdaVectorStore:\n",
    "    def __init__(self, \n",
    "            langda_db_name:Literal[\"Prolog_builtins\", \"Problog_builtins\", \"Available_libraries\"]):\n",
    "        self.problog_official_doc_path = paths.get_absproj_path(\"problog-readthedocs-io-en-latest.pdf\")\n",
    "        self.langda_db_dir = paths.get_absproj_path(\"faiss_langda_kb\")\n",
    "        self.index_name=langda_db_name\n",
    "        self.index_file = self.langda_db_dir / f\"{self.index_name}.faiss\"\n",
    "        self.embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        # Enhanced section metadata with descriptions and keywords Supported_and_unsupported_Prolog_builtins ProbLog_specific_builtins Available_libraries\n",
    "        self.section_metadata = {\n",
    "            '4.1': {\n",
    "                'title': 'Prolog_builtins',\n",
    "                'description': '''Standard Prolog predicates supported or unsupported by ProbLog (based on Yap Prolog):\n",
    "            - Control flow (true, fail, conjunction, disjunction, negation, cut, conditional statements, etc.)\n",
    "            - Term manipulation (var, atom, functor, arg, etc.)\n",
    "            - Arithmetic operations (+, -, *, /, and other mathematical functions)\n",
    "            - Term comparison (==, \\\\==, @<, sort, etc.)\n",
    "            - other builtins...''',\n",
    "                'keywords': ['prolog', 'builtin', 'predicate', 'standard', 'yap'],\n",
    "                'summary': 'This section covers standard Prolog predicates that are supported in ProbLog, including control predicates, term manipulation, arithmetic operations, and more.'\n",
    "            },\n",
    "            '4.2': {\n",
    "                'title': 'Problog_builtins',\n",
    "                'description': '''Special predicates unique to ProbLog for probabilistic logic programming:\n",
    "            - subquery/2, subquery/3: Probabilistic queries\n",
    "            - debugprint/N, error/N: Debugging\n",
    "            - nocache/2: Cache control\n",
    "            - scope operations\n",
    "            - other builtins...''',\n",
    "                'keywords': ['problog', 'builtin', 'probabilistic', 'subquery', 'evidence', 'cache', 'scope'],\n",
    "                'summary': 'This section covers ProbLog-specific predicates for handling probabilistic queries, evidence, debugging, and other features unique to probabilistic logic programming.'\n",
    "            },\n",
    "            '4.3': {\n",
    "                'title': 'Available_libraries',\n",
    "                'description': '''Additional libraries that extend ProbLog functionality:\n",
    "            - Lists: Standard list operations (member, append, reverse)\n",
    "            - Aggregate: LDL++ style aggregation (sum, avg, min, max)\n",
    "            - DB: Database and CSV access\n",
    "            - Cut: Soft cut for ordered rules\n",
    "            - Assert: Dynamic fact management\n",
    "            - Scope: Theory management\n",
    "            - other libraries...''',\n",
    "                'keywords': ['library', 'lists', 'aggregate', 'database', 'string', 'cut', 'assert'],\n",
    "                'summary': 'This section describes various libraries available in ProbLog that provide additional functionality like list operations, aggregation, database access, and more.'\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.subsection_metadata = {\n",
    "            '4.1.1': {\n",
    "                'title': 'Control predicates',\n",
    "                'description': 'Flow control predicates like conjunction, disjunction, negation, ...',\n",
    "                'keywords': ['control', 'flow', 'conjunction', 'disjunction', 'fail', 'true', 'false', 'not', 'call', 'once', 'cut', 'conditional statements'],\n",
    "                'examples': ['P, Q', 'P; Q', 'true/0', 'fail/0', '\\\\+/1', 'call/1','!/0','P -> Q','P *-> Q']\n",
    "            },\n",
    "            '4.1.2': {\n",
    "                'title': 'Handling Undefined Procedures',\n",
    "                'description': 'How ProbLog handles undefined predicates',\n",
    "                'keywords': ['undefined', 'procedure', 'unknown', 'fail'],\n",
    "                'examples': ['unknown(fail)']\n",
    "            },\n",
    "            '4.1.3': {\n",
    "                'title': 'Message Handling',\n",
    "                'description': 'Message handling predicates (not supported)',\n",
    "                'keywords': ['message handling'],\n",
    "                'examples': []\n",
    "            },\n",
    "            '4.1.4': {\n",
    "                'title': 'Predicates on Terms',\n",
    "                'description': 'Predicates for term inspection and manipulation',\n",
    "                'keywords': ['term', 'var', 'atom', 'compound', 'functor', 'arg', 'unification', 'is_list'],\n",
    "                'examples': ['var/1', 'atom/1', 'compound/1', 'float/1', 'arg/3', 'X = Y', 'X \\\\= Y','is_list/1','T1 =@= T2']\n",
    "            },\n",
    "            '4.1.5': {\n",
    "                'title': 'Predicates on Atoms',\n",
    "                'description': 'Atom manipulation predicates (not supported)',\n",
    "                'keywords': ['atom', 'not supported'],\n",
    "                'examples': []\n",
    "            },\n",
    "            '4.1.6': {\n",
    "                'title': 'Predicates on Characters',\n",
    "                'description': 'Character manipulation predicates (not supported)',\n",
    "                'keywords': ['character', 'not supported'],\n",
    "                'examples': []\n",
    "            },\n",
    "            '4.1.7': {\n",
    "                'title': 'Comparing Terms',\n",
    "                'description': 'Term comparison predicates',\n",
    "                'keywords': ['compare', 'equality', 'sort', 'length', 'ordering'],\n",
    "                'examples': ['compare/3', 'X == Y', 'X \\\\== Y', 'X @< Y', 'sort/2', 'length/2']\n",
    "            },\n",
    "            '4.1.8': {\n",
    "                'title': 'Arithmetic',\n",
    "                'description': 'Arithmetic operations and mathematical functions',\n",
    "                'keywords': ['arithmetic', 'math', 'calculation', 'plus', 'minus', 'multiply', 'divide', 'exp', 'log', 'sin', 'cos', 'sqrt'],\n",
    "                'examples': ['X+Y', 'X-Y', 'X*Y', 'X/Y', 'X//Y','exp/1', 'log/1', 'sin/1', 'cos/1', 'X is Y','[X]','random/0']\n",
    "            },\n",
    "            '4.1.9': {\n",
    "                'title': 'Remaining sections',\n",
    "                'description': 'Other Prolog sections not supported',\n",
    "                'keywords': ['remaining', 'not supported'],\n",
    "                'examples': []\n",
    "            },\n",
    "            '4.2': {\n",
    "                'title': 'Problog_specific_builtins',\n",
    "                'description': 'Special predicates unique to ProbLog for probabilistic logic programming',\n",
    "                'keywords': ['problog', 'builtin', 'probabilistic', 'subquery', 'evidence', 'cache', 'scope'],\n",
    "                'examples': ['try_call/N', 'append/3', 'write/N', 'error/N', 'clause/2', 'clause/3']\n",
    "            },\n",
    "            '4.3.1': {\n",
    "                'title': 'Lists',\n",
    "                'description': 'List manipulation predicates from SWI-Prolog',\n",
    "                'keywords': ['list', 'member', 'append', 'select', 'reverse', 'sort', 'permutation'],\n",
    "                'examples': ['member/2', 'append/3', 'select/3', 'reverse/2', 'permutation/2', 'flatten/2']\n",
    "            },\n",
    "            '4.3.2': {\n",
    "                'title': 'Apply',\n",
    "                'description': 'Higher-order predicates for list processing',\n",
    "                'keywords': ['apply', 'maplist', 'foldl', 'include', 'exclude', 'partition'],\n",
    "                'examples': ['maplist/2', 'maplist/3', 'foldl/4', 'include/3', 'exclude/3']\n",
    "            },\n",
    "            '4.3.3': {\n",
    "                'title': 'Cut',\n",
    "                'description': 'Soft cut implementation for ordered rulesets',\n",
    "                'keywords': ['cut', 'soft cut', 'ordered', 'rules', 'indexed clauses'],\n",
    "                'examples': ['cut/2']\n",
    "            },\n",
    "            '4.3.4': {\n",
    "                'title': 'Assert',\n",
    "                'description': 'Dynamic fact assertion and retraction',\n",
    "                'keywords': ['assert', 'retract', 'dynamic', 'database', 'assertz', 'retractall'],\n",
    "                'examples': ['assertz/1', 'retract/1', 'retractall/1']\n",
    "            },\n",
    "            '4.3.5': {\n",
    "                'title': 'Record',\n",
    "                'description': 'Non-backtrackable storage access',\n",
    "                'keywords': ['record', 'storage', 'non-backtrackable', 'database'],\n",
    "                'examples': ['recorda/2', 'recordz/2', 'recorded/2', 'erase/1']\n",
    "            },\n",
    "            '4.3.6': {\n",
    "                'title': 'Aggregate',\n",
    "                'description': 'LDL++ style aggregation operations',\n",
    "                'keywords': ['aggregate', 'sum', 'avg', 'min', 'max', 'group by'],\n",
    "                'examples': ['sum<X>', 'avg<X>', 'min<X>', 'max<X>']\n",
    "            },\n",
    "            '4.3.7': {\n",
    "                'title': 'Collect',\n",
    "                'description': 'Generalized aggregation with => operator',\n",
    "                'keywords': ['collect', 'aggregation', 'group by', '=>'],\n",
    "                'examples': ['(CODE) => GroupBy / AggFunc']\n",
    "            },\n",
    "            '4.3.8': {\n",
    "                'title': 'DB',\n",
    "                'description': 'SQLite database and CSV file access',\n",
    "                'keywords': ['database', 'sqlite', 'csv', 'sql', 'data access'],\n",
    "                'examples': ['sqlite_load/1', 'sqlite_csv/2']\n",
    "            },\n",
    "            '4.3.9': {\n",
    "                'title': 'Scope',\n",
    "                'description': 'Managing multiple ProbLog theories with scopes',\n",
    "                'keywords': ['scope', 'theory', 'namespace', 'union', 'conjunction'],\n",
    "                'examples': ['scope(1):predicate', 'scope1:X; scope2:X']\n",
    "            },\n",
    "            '4.3.10': {\n",
    "                'title': 'String',\n",
    "                'description': 'String manipulation predicates',\n",
    "                'keywords': ['string', 'manipulation', 'text processing'],\n",
    "                'examples': []\n",
    "            },\n",
    "            '4.3.11': {\n",
    "                'title': 'NLP4PLP',\n",
    "                'description': 'Library for representing and solving probability questions',\n",
    "                'keywords': ['nlp', 'natural language', 'probability', 'questions'],\n",
    "                'examples': []\n",
    "            }\n",
    "        }\n",
    "    def split_and_save(self, pdf_path, embedding_function, db_dir):\n",
    "        loader = PyMuPDFLoader(str(pdf_path))\n",
    "        page_docs = loader.load()\n",
    "        \n",
    "        current_section = None\n",
    "        current_subsection_key = None\n",
    "        current_subsection_content = []\n",
    "        subsection_list = []\n",
    "        \n",
    "        for page in page_docs:\n",
    "            lines = page.page_content.splitlines()\n",
    "            \n",
    "            for line in lines[1:-2]:\n",
    "                # Try to match a section number (4.1, 4.2, 4.3) or a subsection number (4.1.1, 4.1.2, etc.)\n",
    "                match = re.match(r'^(4\\.\\d+(?:\\.\\d+)?)\\s', line)\n",
    "                \n",
    "                if match:\n",
    "                    section_key = match.group(1)\n",
    "                    \n",
    "                    # Check if it is a main section (4.1, 4.2, 4.3)\n",
    "                    if section_key in self.section_metadata:\n",
    "                        # Save the current section content\n",
    "                        if current_subsection_key and current_subsection_content:\n",
    "                            doc = Document(\n",
    "                                page_content=\"\\n\".join(current_subsection_content),\n",
    "                                metadata={\n",
    "                                    **self.subsection_metadata.get(current_subsection_key, {})\n",
    "                                }\n",
    "                            )\n",
    "                            subsection_list.append(doc)\n",
    "                        \n",
    "                        # Save all sub-sections of the current section to the vector database\n",
    "                        if current_section and subsection_list:\n",
    "                            vector_store = FAISS.from_documents(subsection_list, embedding_function)\n",
    "                            vector_store.save_local(db_dir, index_name=current_section['title'])\n",
    "                            subsection_list = []\n",
    "                        \n",
    "                        # Start a new chapter\n",
    "                        current_section = self.section_metadata[section_key]\n",
    "                        current_subsection_key = None\n",
    "                        current_subsection_content = []\n",
    "                    \n",
    "                    # Check if it is a subsection\n",
    "                    if section_key in self.subsection_metadata:\n",
    "                        # Save the current subsection content\n",
    "                        if current_subsection_key and current_subsection_content:\n",
    "                            doc = Document(\n",
    "                                page_content=\"\\n\".join(current_subsection_content),\n",
    "                                metadata={\n",
    "                                    **self.subsection_metadata.get(current_subsection_key, {})\n",
    "                                }\n",
    "                            )\n",
    "                            subsection_list.append(doc)\n",
    "                        \n",
    "                        # Start a new chapter\n",
    "                        current_subsection_key = section_key\n",
    "                        current_subsection_content = [line]\n",
    "                else:\n",
    "                    # Normal\n",
    "                    if current_subsection_key:\n",
    "                        current_subsection_content.append(line)\n",
    "        \n",
    "        # handling the last chapter\n",
    "        if current_subsection_key and current_subsection_content:\n",
    "            doc = Document(\n",
    "                page_content=\"\\n\".join(current_subsection_content),\n",
    "                metadata={\n",
    "                    **self.subsection_metadata.get(current_subsection_key, {})\n",
    "                }\n",
    "            )\n",
    "            subsection_list.append(doc)\n",
    "        \n",
    "        # Save the last chapter content\n",
    "        if current_section and subsection_list:\n",
    "            vector_store = FAISS.from_documents(subsection_list, embedding_function)\n",
    "            vector_store.save_local(db_dir, index_name=current_section['title'])\n",
    "\n",
    "        print(f\"Successfully created 3 vector databases：\")\n",
    "        for section_key, section_info in self.section_metadata.items():\n",
    "            print(f\"  - {section_info['title']}\")\n",
    "\n",
    "\n",
    "    @property\n",
    "    def vs(self,\n",
    "        ) -> FAISS:\n",
    "        \"\"\"\n",
    "        Get vector storage object\n",
    "        returns:\n",
    "            vector database\n",
    "        \"\"\"\n",
    "        if not self.index_file.exists():\n",
    "            self.split_and_save(self.problog_official_doc_path, self.embedding_function, self.langda_db_dir)\n",
    "\n",
    "        return FAISS.load_local(\n",
    "            self.langda_db_dir, \n",
    "            self.embedding_function, \n",
    "            index_name=self.index_name,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def get_chapter_metadata(self) -> dict:\n",
    "        \"\"\"\n",
    "        get metadata of a chapter\n",
    "        \"\"\"\n",
    "        for key, value in self.section_metadata.items():\n",
    "            if value['title'] == self.index_name:\n",
    "                return value\n",
    "\n",
    "    def similarity_search(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform similarity search on the local vector library.\n",
    "        args:\n",
    "            query: search query text\n",
    "            k: number of results to return\n",
    "        returns:\n",
    "            document list\n",
    "        \"\"\"\n",
    "        return self.vs.similarity_search(query, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0a62a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, List, Optional, Type, ClassVar\n",
    "from problog.program import PrologString\n",
    "from problog import get_evaluatable, evaluator\n",
    "\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.callbacks.manager import CallbackManagerForToolRun\n",
    "\n",
    "# from config import paths\n",
    "prolog_builtins = LangdaVectorStore(\"Prolog_builtins\")\n",
    "problog_builtins = LangdaVectorStore(\"Problog_builtins\")\n",
    "available_libraries = LangdaVectorStore(\"Available_libraries\")\n",
    "source_map = {\n",
    "    \"Prolog_builtins\":prolog_builtins,\n",
    "    \"Problog_builtins\":problog_builtins,\n",
    "    \"Available_libraries\":available_libraries,\n",
    "}\n",
    "source_metadata_map = {\n",
    "    \"Prolog_builtins\":prolog_builtins.section_metadata['4.1'],\n",
    "    \"Problog_builtins\":problog_builtins.section_metadata['4.2'],\n",
    "    \"Available_libraries\":available_libraries.section_metadata['4.3'],\n",
    "}\n",
    "class RetrieverInput(BaseModel):\n",
    "    query: str = Field(description=\"The query string to search for in the knowledge base\")\n",
    "\n",
    "class RetrieverTool(BaseTool):\n",
    "    name: ClassVar[str] = \"retriever_tool\"\n",
    "    description: ClassVar[str] = f\"\"\"Useful for searching the ProbLog official documentation for syntax and predicate informations.\"\"\"\n",
    "    args_schema: Type[BaseModel] = RetrieverInput\n",
    "    source_name: str\n",
    "\n",
    "    def _run(self, \n",
    "            query: str, \n",
    "            run_manager: Optional[CallbackManagerForToolRun] = None) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Run the retriever tool to get relevant documents from the vector store.\n",
    "        \"\"\"\n",
    "        print(\" Running retriever tool...\")\n",
    "        # Perform similarity search\n",
    "        try:\n",
    "            vector_store = source_map[self.source_name]\n",
    "            docs = vector_store.similarity_search(query, k=3)\n",
    "            return self._format_results(docs, self.source_name)\n",
    "        except:\n",
    "            return {\"error\": f\"Source not found: {self.source_name}\"}\n",
    "\n",
    "    def _format_results(self, docs:List[Document], source_name: str) -> List[dict]:\n",
    "        \"\"\"Format the search results with source information.\"\"\"\n",
    "        formatted = []\n",
    "        for i, doc in enumerate(docs):\n",
    "            formatted.append({\n",
    "                \"index\": i,\n",
    "                \"source\": source_name,\n",
    "                \"content\": doc.page_content,\n",
    "            })\n",
    "        return formatted\n",
    "        \n",
    "class RetrieverToolFactory:\n",
    "    \"\"\"Factory class to create retriever tools for different vector stores\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_retriever_tool(source_name:str) -> RetrieverTool:\n",
    "        \"\"\"Create a retriever tool for Prolog builtins\"\"\"\n",
    "        tool = RetrieverTool(\n",
    "            name = f\"{source_name}_retriever_tool\",\n",
    "            description = f\"\"\"Retrieve information about Prolog built-in predicates and functionality:\n",
    "        {source_metadata_map[source_name]}\"\"\",\n",
    "            source_name=source_name)\n",
    "\n",
    "        return tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc42463",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOL_REGISTRY = {\n",
    "    \"Prolog_builtins_retriever_tool\": RetrieverToolFactory.create_retriever_tool(\"Prolog_builtins\"),\n",
    "    \"Problog_builtins_retriever_tool\": RetrieverToolFactory.create_retriever_tool(\"Problog_builtins\"),\n",
    "    \"Available_libraries_retriever_tool\": RetrieverToolFactory.create_retriever_tool(\"Available_libraries\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1287607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running retriever tool...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'index': 0,\n",
       "  'source': 'Prolog_builtins',\n",
       "  'content': '4.1.1 Control predicates\\nSupported:\\n• P, Q\\n• P; Q\\n• true/0\\n• fail/0\\n• false/0\\n• \\\\+/1\\n• not/1\\n• call/1\\n• call/N (for N up to 9)\\n• forall/2\\nSpecial:\\n• once/1: In ProbLog once/1 is an alias for call/1.\\nNot supported:\\n• !/0\\n• P -> Q\\n• P *-> Q\\n• repeat\\n• incore/1 (use call/1)\\n• call_with_args/N (use call/N)\\n• if(A,B,C) (use (A,B);(\\\\+A,C))\\n• ignore/1\\n• abort/0\\n• break/0\\n• halt/0\\n• halt/1\\n• catch/3\\n• throw/1\\n• garbage_collect/0\\n• garbage_collect_atoms/0\\n• gc/0\\n• nogc/0\\n• grow_heap/1\\n• grow_stack/1'},\n",
       " {'index': 1,\n",
       "  'source': 'Prolog_builtins',\n",
       "  'content': '4.1.3 Message Handling\\nNot supported: all'},\n",
       " {'index': 2,\n",
       "  'source': 'Prolog_builtins',\n",
       "  'content': '4.1.2 Handling Undefined Procedures\\nAlternative:\\n• unknown(fail) can be used\\nNot supported: all'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool1 = RetrieverToolFactory.create_retriever_tool(\"Prolog_builtins\")\n",
    "tool1._run(\"ProbLog lists module\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de45fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Literal, List\n",
    "from config import paths\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# ======================================================================== #\n",
    "#                                Retriever                                 #\n",
    "# ======================================================================== #\n",
    "class LangdaVectorStore:\n",
    "    \"\"\"\n",
    "    Creates and manages a FAISS vector store from JSON data\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.json_dir = paths.get_abscase_path(\"utils\")\n",
    "        self.vs_dir = self.json_dir / \"vector_store\"\n",
    "        self.vs_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.index_name=\"problog_syntax_docs\"\n",
    "        self.json_file_path = self.json_dir / f\"{self.index_name}.json\"\n",
    "        self.vector_store_path = self.vs_dir / f\"{self.index_name}.faiss\"\n",
    "        self.embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "    def create_documents(self) -> List[Document]:\n",
    "        \"\"\"\n",
    "        convert JSON data to LangChain Documents\n",
    "        returns:\n",
    "            List of Document objects ready for vectorization\n",
    "        \"\"\"\n",
    "        documents:List[Document] = []\n",
    "        try:\n",
    "            with open(self.json_file_path, 'r', encoding='utf-8') as f:\n",
    "                json_file:List[dict] = json.load(f)\n",
    "            print(f\"Successfully loaded {len(json_file)} items from {self.json_file_path}\")\n",
    "\n",
    "            for item in json_file:\n",
    "                content = item.get(item.get('content', ''))\n",
    "\n",
    "                if not content:\n",
    "                    print(f\"Warning: No content found for item with id: {item.get('id', 'unknown')}\")\n",
    "                    continue\n",
    "\n",
    "                # Create metadata from other fields\n",
    "                metadata = {\n",
    "                    'id': item.get('id', ''),\n",
    "                    'title': item.get('title', ''),\n",
    "                    'tags': item.get('tags', []),\n",
    "                    'keywords': item.get('keywords', []),\n",
    "                    # Add any other fields as metadata\n",
    "                }\n",
    "\n",
    "                doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                documents.append(doc)\n",
    "\n",
    "            print(f\"Created {len(documents)} documents from JSON data\")\n",
    "            return documents\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating Documents file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_faiss_vector_store(self) -> FAISS:\n",
    "        documents = self.create_documents()\n",
    "        if not documents:\n",
    "            raise ValueError(\"No documents found to create vector store\")\n",
    "        vector_store = FAISS.from_documents(documents, self.embedding_function)\n",
    "        vector_store.save_local(self.vs_dir, index_name=self.index_name)\n",
    "        print(f\"Vector store saved to {self.vs_dir}/{self.index_name}\")\n",
    "\n",
    "    @property\n",
    "    def vs(self,\n",
    "        ) -> FAISS:\n",
    "        \"\"\"\n",
    "        Get or create the vector storage object\n",
    "        returns:\n",
    "            FAISS vector store object\n",
    "        \"\"\"\n",
    "        if not self.vector_store_path.exists():\n",
    "            self.create_faiss_vector_store()\n",
    "\n",
    "        return FAISS.load_local(\n",
    "            self.vs_dir, \n",
    "            self.embedding_function, \n",
    "            index_name=self.index_name,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "    def similarity_search(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform similarity search on the local vector library.\n",
    "        args:\n",
    "            query: search query text\n",
    "            k: number of results to return\n",
    "        returns:\n",
    "            document list\n",
    "        \"\"\"\n",
    "        return self.vs.similarity_search(query, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45d934c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = LangdaVectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03d77362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 32 items from /Users/zhenzhili/MASTERTHESIS/#Expert_System_Design/examples/LANGDA/#drone_with_dpl_in_langgraph_v6.4/utils/problog_syntax_docs.json\n",
      "Warning: No content found for item with id: syntax_probabilistic_facts\n",
      "Warning: No content found for item with id: syntax_annotated_disjunctions\n",
      "Warning: No content found for item with id: syntax_probabilistic_clauses\n",
      "Warning: No content found for item with id: syntax_deterministic_rules\n",
      "Warning: No content found for item with id: syntax_queries\n",
      "Warning: No content found for item with id: syntax_evidence\n",
      "Warning: No content found for item with id: syntax_tabling\n",
      "Warning: No content found for item with id: syntax_lfi_mode\n",
      "Warning: No content found for item with id: syntax_dt_mode\n",
      "Warning: No content found for item with id: builtin_control_predicates\n",
      "Warning: No content found for item with id: builtin_arithmetic\n",
      "Warning: No content found for item with id: builtin_meta_predicates\n",
      "Warning: No content found for item with id: builtin_predicates_on_terms\n",
      "Warning: No content found for item with id: builtin_predicates_on_atoms\n",
      "Warning: No content found for item with id: builtin_predicates_on_characters\n",
      "Warning: No content found for item with id: builtin_comparing_terms\n",
      "Warning: No content found for item with id: builtin_handling_undefined\n",
      "Warning: No content found for item with id: builtin_message_handling\n",
      "Warning: No content found for item with id: builtin_problog_specific\n",
      "Warning: No content found for item with id: library_lists\n",
      "Warning: No content found for item with id: library_apply\n",
      "Warning: No content found for item with id: library_cut\n",
      "Warning: No content found for item with id: library_assert\n",
      "Warning: No content found for item with id: library_record\n",
      "Warning: No content found for item with id: library_aggregate\n",
      "Warning: No content found for item with id: library_collect\n",
      "Warning: No content found for item with id: library_db\n",
      "Warning: No content found for item with id: library_scope\n",
      "Warning: No content found for item with id: library_string\n",
      "Warning: No content found for item with id: library_nlp4plp\n",
      "Warning: No content found for item with id: example_smoking_influence\n",
      "Warning: No content found for item with id: example_burglary_alarm\n",
      "Created 0 documents from JSON data\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhat is the ->\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mLangdaVectorStore.similarity_search\u001b[39m\u001b[34m(self, query, k)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, k: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m) -> List[Document]:\n\u001b[32m     92\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[33;03m    Perform similarity search on the local vector library.\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[33;03m    args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m \u001b[33;03m        document list\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvs\u001b[49m.similarity_search(query, k=k)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mLangdaVectorStore.vs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[33;03mGet or create the vector storage object\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03mreturns:\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m    FAISS vector store object\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vector_store_path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_faiss_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m FAISS.load_local(\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.vs_dir, \n\u001b[32m     86\u001b[39m     \u001b[38;5;28mself\u001b[39m.embedding_function, \n\u001b[32m     87\u001b[39m     index_name=\u001b[38;5;28mself\u001b[39m.vs_index_name,\n\u001b[32m     88\u001b[39m     allow_dangerous_deserialization=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     89\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mLangdaVectorStore.create_faiss_vector_store\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_faiss_vector_store\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> FAISS:\n\u001b[32m     68\u001b[39m     documents = \u001b[38;5;28mself\u001b[39m.create_documents()\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     vector_store = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     vector_store.save_local(\u001b[38;5;28mself\u001b[39m.vs_dir, index_name=\u001b[38;5;28mself\u001b[39m.vs_index_name)\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVector store saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.vs_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.vs_index_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/langda/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:848\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    846\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/langda/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:1044\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1043\u001b[39m embeddings = embedding.embed_documents(texts)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/langda/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:1001\u001b[39m, in \u001b[36mFAISS.__from\u001b[39m\u001b[34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[39m\n\u001b[32m    998\u001b[39m     index = faiss.IndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[32m0\u001b[39m]))\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1000\u001b[39m     \u001b[38;5;66;03m# Default to L2, currently other metric types not initialized.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     index = faiss.IndexFlatL2(\u001b[38;5;28mlen\u001b[39m(\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[32m   1002\u001b[39m docstore = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdocstore\u001b[39m\u001b[33m\"\u001b[39m, InMemoryDocstore())\n\u001b[32m   1003\u001b[39m index_to_docstore_id = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mindex_to_docstore_id\u001b[39m\u001b[33m\"\u001b[39m, {})\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "results = vs.similarity_search(\"what is the ->\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "656bf26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from config import paths\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# ======================================================================== #\n",
    "#                                Retriever                                 #\n",
    "# ======================================================================== #\n",
    "class LangdaVectorStore:\n",
    "    \"\"\"\n",
    "    Creates and manages a FAISS vector store from JSON data\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.json_dir = paths.get_abscase_path(\"utils\")\n",
    "        self.vs_dir = self.json_dir / \"vector_store\"\n",
    "        # 创建目录如果不存在\n",
    "        self.vs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.index_name = \"problog_syntax_docs\"\n",
    "        self.json_file_path = self.json_dir / f\"{self.index_name}.json\"\n",
    "\n",
    "        # 修复：使用正确的属性名\n",
    "        self.vs_index_name = self.index_name  # 移除.faiss扩展名，因为FAISS会自动添加\n",
    "        self.vector_store_path = self.vs_dir / f\"{self.vs_index_name}.faiss\"  # 修复属性名\n",
    "        self.embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "    def create_documents(self) -> List[Document]:\n",
    "        \"\"\"\n",
    "        convert JSON data to LangChain Documents\n",
    "        returns:\n",
    "            List of Document objects ready for vectorization\n",
    "        \"\"\"\n",
    "        documents: List[Document] = []\n",
    "        try:\n",
    "            with open(self.json_file_path, 'r', encoding='utf-8') as f:\n",
    "                json_file: List[dict] = json.load(f)\n",
    "            print(f\"Successfully loaded {len(json_file)} items from {self.json_file_path}\")\n",
    "\n",
    "            for item in json_file:\n",
    "                # 修复：获取content字段的值\n",
    "                # 优先使用embedding_text，如果没有则使用content\n",
    "                content = item.get('embedding_text', item.get('content', ''))\n",
    "\n",
    "                if not content:\n",
    "                    print(f\"Warning: No content found for item with id: {item.get('id', 'unknown')}\")\n",
    "                    continue\n",
    "\n",
    "                # Create metadata from other fields\n",
    "                metadata = {\n",
    "                    'id': item.get('id', ''),\n",
    "                    'title': item.get('title', ''),\n",
    "                    'tags': item.get('tags', []),\n",
    "                    'keywords': item.get('keywords', []),\n",
    "                    # Add any other fields as metadata\n",
    "                }\n",
    "\n",
    "                doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                documents.append(doc)\n",
    "\n",
    "            print(f\"Created {len(documents)} documents from JSON data\")\n",
    "            return documents\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating Documents file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_faiss_vector_store(self) -> FAISS:\n",
    "        \"\"\"\n",
    "        Create and save FAISS vector store\n",
    "        returns:\n",
    "            FAISS vector store object\n",
    "        \"\"\"\n",
    "        print(\"Creating FAISS vector store...\")\n",
    "        documents = self.create_documents()\n",
    "        \n",
    "        if not documents:\n",
    "            raise ValueError(\"No documents found to create vector store\")\n",
    "        \n",
    "        vector_store = FAISS.from_documents(documents, self.embedding_function)\n",
    "        vector_store.save_local(self.vs_dir, index_name=self.vs_index_name)\n",
    "        print(f\"Vector store saved to {self.vs_dir}/{self.vs_index_name}\")\n",
    "        return vector_store\n",
    "\n",
    "    @property\n",
    "    def vs(self) -> FAISS:\n",
    "        \"\"\"\n",
    "        Get or create the vector storage object\n",
    "        returns:\n",
    "            FAISS vector store object\n",
    "        \"\"\"\n",
    "        if not self.vector_store_path.exists():\n",
    "            print(\"Vector store not found, creating new one...\")\n",
    "            return self.create_faiss_vector_store()\n",
    "\n",
    "        print(\"Loading existing vector store...\")\n",
    "        return FAISS.load_local(\n",
    "            self.vs_dir, \n",
    "            self.embedding_function, \n",
    "            index_name=self.vs_index_name,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "    def similarity_search(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform similarity search on the local vector library.\n",
    "        args:\n",
    "            query: search query text\n",
    "            k: number of results to return\n",
    "        returns:\n",
    "            document list\n",
    "        \"\"\"\n",
    "        return self.vs.similarity_search(query, k=k)\n",
    "\n",
    "    def similarity_search_with_scores(self, query: str, k: int = 5) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        Perform similarity search with similarity scores\n",
    "        args:\n",
    "            query: Search query text\n",
    "            k: Number of results to return\n",
    "            \n",
    "        returns:\n",
    "            List of (document, score) tuples\n",
    "        \"\"\"\n",
    "        return self.vs.similarity_search_with_score(query, k=k)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d32fb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = LangdaVectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c499adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(id='544fa821-3b4e-4c37-80d8-4a9cab13a267', metadata={'id': 'builtin_problog_specific', 'title': 'ProbLog-Specific Builtins', 'tags': ['predicate', 'problog', 'builtin'], 'keywords': ['subquery', 'try_call', 'debugprint', 'possible']}, page_content=\"ProbLog provides some special built-in predicates specific to its environment:\\n\\n- try_call/N: like call/N (with N arguments) but if the predicate is not defined, it fails without error (useful for optional predicates).\\n- subquery(Goal, Probability): computes the probability of Goal as a subquery and unifies it with Probability (allows querying a probability inside the program).\\n- subquery(Goal, Probability, EvidenceList): computes the probability of Goal given a list of evidence literals, returning the probability.\\n- subquery(Goal, Probability, EvidenceList, Semiring, Evaluator): extended subquery specifying a custom semiring and evaluator by name (advanced usage for different inference strategies).\\n- I/O like printing: debugprint/N (print messages to stderr for debugging), write/N (print to stdout), writenl/N (print with newline), nl/0 (newline output).\\n- Error raising: error/N which throws a runtime user error with given message.\\n- cmd_args/1: retrieve command line arguments passed to ProbLog (when using ProbLog as a script, with -a options).\\n- Conversions: atom_number/2 to convert between an atom and a number.\\n- Caching control: nocache(Functor, Arity): disable tabling (caching) for the predicate with given name/arity (if needed, to force recalculation every time).\\n- Variants of numbervars: numbervars/2 and numbervars/3, and varnumbers/2 for numbering variables (useful in some transformations).\\n- Term subsumption checks: subsumes_term/2 and subsumes_chk/2 (similar to subsumes_term, with one likely returning true/false without bindings).\\n- Deterministic query: possible/1 which can be used to check if a given atom is possibly true (nonzero probability) under the model.\\n- Inspecting the program: clause/2 and clause/3 to retrieve clauses from the program (similar to Prolog's clause/2,3 for dynamic code inspection of the ProbLog program).\\n- Working with scopes (see Scope library): create_scope/2 to create a new named sub-theory, subquery_in_scope(Scope, Goal, Probability, ...) variants to query probabilities within a specific scope, and call_in_scope/N to call goals within a scope.\\n- State management: set_state/1, reset_state/0, check_state/1, print_state/0 for advanced users to manipulate the solver state or sequences.\\n- Sequence generation: seq/1 which unifies its argument with a new unique sequential number each time it is called (useful for generating unique IDs).\"),\n",
       "  421.2859),\n",
       " (Document(id='df06573c-98d0-4fed-89bf-69f6eee029eb', metadata={'id': 'library_lists', 'title': 'Lists Library (library(lists))', 'tags': ['library', 'lists'], 'keywords': ['member', 'append', 'reverse', 'flatten']}, page_content=\"The Lists library (library(lists)) provides list-processing predicates. It implements all predicates from SWI-Prolog's standard lists library, including:\\nmemberchk/2, member/2, append/3 (and append/2), prefix/2, select/3 (and selectchk/3 variants, and 4-argument versions), nextto/3, delete/3, nth0/3, nth1/3 (and their 4-argument forms), last/2, proper_length/2, same_length/2, reverse/2, permutation/2, flatten/2, max_member/2, min_member/2, sum_list/2, max_list/2, min_list/2, numlist/3, is_set/1, list_to_set/2, intersection/3, union/3, subset/2, subtract/3.\\n\\nIn addition to these standard predicates, ProbLog's lists library provides some extra predicates useful in probabilistic modeling:\\n- select_uniform(+ID, +Values, ?Value, ?Rest): Selects one element uniformly at random from Values, unifying Value with the chosen element and Rest with the remaining list. The ID can be used to tie multiple selections together (so that the same random choice is reused in multiple places if the ID matches).\\n- select_weighted(+ID, +Weights, +Values, ?Value, ?Rest): Selects one element from Values with probability proportional to the corresponding weight in Weights. Works similarly to select_uniform but uses a list of weights for a weighted random choice.\\n- groupby(?List, ?Groups): Partitions a list of terms into groups of identical terms (unifying Groups with a list of groups).\\n- sub_list(?List, ?Before, ?Length, ?After, ?SubList): Extracts a sub-list of length Length from List such that SubList appears with Before elements before it and After elements after it in the original list.\\n- enum_groups(+Groups, +Values, -Group, -GroupedValues) (and its variant enum_groups(+GroupedValues, -Group, -GroupedValues)): Enumerate group names and their associated values from a grouping structure.\\n- unzip(ListAB, ListA, ListB): Splits a list of pair terms into two lists, one with all first components and one with all second components.\\n- zip(ListA, ListB, ListAB): The inverse of unzip; pairs corresponding elements from ListA and ListB into a list of pair terms ListAB.\\n- make_list(+Len, +Elem, ?List): Creates a list of length Len filled with copies of Elem (or checks that a given List has that form).\"),\n",
       "  437.34708),\n",
       " (Document(id='5254b66a-6155-472c-a543-4ec663fd2692', metadata={'id': 'builtin_predicates_on_characters', 'title': 'Predicates on Characters', 'tags': ['predicate', 'character', 'builtin'], 'keywords': ['char_code', 'character']}, page_content=\"Predicates dealing with character codes or character classification (for example, checking if a character is a digit or converting between characters and character codes) are not supported in ProbLog. Essentially, no built-in predicates for character manipulation (like those in Prolog's char_code/2 or character classification routines) are available at this time.\"),\n",
       "  449.73276),\n",
       " (Document(id='77bf2238-8cf8-4a64-b475-397e809884ba', metadata={'id': 'builtin_comparing_terms', 'title': 'Comparing Terms', 'tags': ['predicate', 'compare', 'builtin'], 'keywords': ['compare', '==', 'sort', 'length']}, page_content='ProbLog supports term comparison predicates for checking term order and equality beyond unification:\\n\\nSupported comparison predicates:\\n- compare/3 (total ordering of terms, returns comparison result as <, =, or >).\\n- Term identity: X == Y (true if X and Y are exactly the same term) and X \\\\== Y (true if they are not the same term).\\n- Term ordering: X @< Y, X @=< Y, X @> Y, X @>= Y (lexicographic standard order comparisons for terms).\\n- sort/2 (sort a list, removing duplicates) and length/2 (get or constrain the length of a list). (Note: In ProbLog, calling length(List, L) with both List and L unbound is not allowed.)\\n\\nNot supported:\\n- keysort/2 and predsort/3 (sorting with custom keys or predicates) are not available.'),\n",
       "  459.6863),\n",
       " (Document(id='54ca7e47-e6ae-4e48-b196-ed166181924e', metadata={'id': 'builtin_message_handling', 'title': 'Message Handling', 'tags': ['predicate', 'exception', 'builtin'], 'keywords': ['catch', 'throw', 'error']}, page_content=\"Prolog's message handling predicates (for catching and throwing exceptions or printing system messages) are not available in ProbLog's modeling language. Constructs like catch/3, throw/1, and other built-in message or exception handling predicates are not supported in ProbLog.\"),\n",
       "  460.5915)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs.similarity_search_with_scores(\"should I use if else'->'?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
