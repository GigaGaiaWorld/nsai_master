{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190e23a8",
   "metadata": {},
   "source": [
    "### in this example, we try to intergrate another LLM that call langda as a module to solve crusial problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc4491",
   "metadata": {},
   "source": [
    "principle:\n",
    "- Question + langda code with \"/* CONTENT */\" placeholder only => LLM_MAIN: decide what to fill in each placeholder\n",
    "- fill the content => langda_solve => complete code => result\n",
    "- LLM_MAIN answer the Question with the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c992f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langda import langda_solve, invoke_agent\n",
    "from dotenv import find_dotenv\n",
    "\n",
    "task_solve_template = \"\"\"\n",
    "% about burglary\n",
    "langda(LLM:\"/* BURGLARY */\").\n",
    "% about earthquake\n",
    "langda(LLM:\"/* EARTHQUAKE */\").\n",
    "\n",
    "% about case 1\n",
    "langda(LLM:\"/* ALARM1 */\",FUP:\"false\").\n",
    "% about case 2\n",
    "langda(LLM:\"/* ALARM2 */\",FUP:\"false\").\n",
    "\n",
    "% law for alarm\n",
    "alarm :- langda(LLM:\"/* CONTENT3 */\",FUP:\"false\").\n",
    "query(alarm).\n",
    "\"\"\"\n",
    "\n",
    "task_solve_template2 = \"\"\"\n",
    "% about burglary\n",
    "\n",
    "% about earthquake\n",
    "\n",
    "% about case 1\n",
    "\n",
    "% about case 2\n",
    "\n",
    "% law for alarm\n",
    "\n",
    "query(alarm).\n",
    "\"\"\"\n",
    "\n",
    "class LLMConfig(BaseModel):\n",
    "    model: str\n",
    "    api_key: str\n",
    "    api_typ: str = \"Bearer\"\n",
    "    api_ver: str = \"2024-01-01\"\n",
    "    temperature: float = 0.2\n",
    "\n",
    "class AgentSettings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=find_dotenv(),\n",
    "        env_file_encoding=\"utf-8\",\n",
    "        env_nested_delimiter=\"_\",\n",
    "        env_nested_max_split=1,\n",
    "        extra=\"ignore\"\n",
    "    )\n",
    "    deepseek: LLMConfig\n",
    "    openai: LLMConfig\n",
    "    groq: LLMConfig\n",
    "\n",
    "class LangdaAgentExecutor(BaseModel):\n",
    "    cfgs: AgentSettings = Field(default_factory=AgentSettings)    \n",
    "\n",
    "    def solve(self, question) -> str:\n",
    "        system_prompt1 = f\"\"\"You have following template to solve the tasks:\n",
    "<Template>\n",
    "{task_solve_template}\n",
    "</Template>\n",
    "please think step by step, and give content to each placeholder /* CONTENT */, which could save the question. \n",
    "</Instruction>\n",
    "<Final_Answer>\n",
    "Please generate content in following form for each placeholder:\n",
    "\n",
    "```content\n",
    "what the logic is about content 1\n",
    "```\n",
    "```content\n",
    "what the logic is about content 2\n",
    "```\n",
    "\n",
    "If there's no content needed for certain placeholder:\n",
    "```content\n",
    "//nothing\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "        system_prompt2 = f\"\"\"You have following template to solve the tasks:\n",
    "<Template>\n",
    "{task_solve_template2}\n",
    "</Template>\n",
    "please think step by step, and give the full problog code that could solve the task\n",
    "</Instruction>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        chatprompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt2),\n",
    "            (\"human\", question)\n",
    "        ])\n",
    "\n",
    "        llm = ChatDeepSeek(\n",
    "            model=self.cfgs.deepseek.model,\n",
    "            temperature=self.cfgs.deepseek.temperature,\n",
    "            api_key=self.cfgs.deepseek.api_key,\n",
    "        )\n",
    "        \n",
    "        chain = chatprompt_template | llm | StrOutputParser()\n",
    "        \n",
    "        result = chain.invoke({})\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dafa455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this problem, we need to model the given probabilities and conditions in ProbLog. Here's the step-by-step solution:\n",
      "\n",
      "1. Define the probabilities of burglary and earthquake:\n",
      "   - burglary with probability 0.8\n",
      "   - earthquake with probability 0.2\n",
      "\n",
      "2. Define the cases when the alarm sounds:\n",
      "   - Case 1: earthquake occurs and no burglary occurs\n",
      "   - Case 2: both burglary and earthquake occur\n",
      "\n",
      "3. The alarm sounds if either Case 1 or Case 2 occurs\n",
      "\n",
      "Here's the complete ProbLog code:\n",
      "\n",
      "```prolog\n",
      "% about burglary\n",
      "0.8::burglary.\n",
      "\n",
      "% about earthquake\n",
      "0.2::earthquake.\n",
      "\n",
      "% about case 1 (earthquake and no burglary)\n",
      "case1 :- earthquake, \\+ burglary.\n",
      "\n",
      "% about case 2 (both burglary and earthquake)\n",
      "case2 :- burglary, earthquake.\n",
      "\n",
      "% law for alarm\n",
      "alarm :- case1.\n",
      "alarm :- case2.\n",
      "\n",
      "query(alarm).\n",
      "```\n",
      "\n",
      "This code models:\n",
      "- The independent probabilities of burglary (0.8) and earthquake (0.2)\n",
      "- The two cases when the alarm sounds:\n",
      "  1. When there's an earthquake but no burglary (case1)\n",
      "  2. When both burglary and earthquake occur (case2)\n",
      "- The alarm sounds if either case1 or case2 is true\n",
      "- Finally, we query the probability of the alarm sounding\n",
      "\n",
      "The probability of the alarm being sounded will be calculated as:\n",
      "P(alarm) = P(case1) + P(case2) = P(earthquake ∧ ¬burglary) + P(burglary ∧ earthquake)\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "The probability of a burglary is 0.8, and the probability of an earthquake is 0.2. \n",
    "If we have the following situations:\n",
    " 1. When an earthquake occurs but no burglary occurs, an alarm is sounded\n",
    " 2. When both a burglary and an earthquake occur, an alarm is sounded\n",
    "What is the probability of an alarm being sounded?\n",
    "\"\"\"\n",
    "\n",
    "executor = LangdaAgentExecutor()\n",
    "result = executor.solve(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6268f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_blocks(text: str) -> List[str]:\n",
    "    pattern = r'```content\\s*\\n(.*?)\\n```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    content_blocks = [match.strip() for match in matches]\n",
    "    return content_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa04bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
